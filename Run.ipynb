{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/drafting/blob/main/Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#frameworks and packages"
      ],
      "metadata": {
        "id": "cFv1ZgY33dkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UCPc6Qy2lsnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f5af0a-063d-4180-cfb3-c59caacd0ceb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LEARNING RATE"
      ],
      "metadata": {
        "id": "awvygFnuTsRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #run.sh"
      ],
      "metadata": {
        "id": "43k7wpu34m4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr = 0.5"
      ],
      "metadata": {
        "id": "Ael2ePIxXEJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr = 0.05"
      ],
      "metadata": {
        "id": "v6fpiiClXNOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 15 \\\n",
        "--lr 0.5 --wd 0.0\\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.5.png \\\n",
        "--test\n"
      ],
      "metadata": {
        "id": "CH2_HF5X4oko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e692b60-156b-4a8b-c16d-b525975addcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to mydata2/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:02<00:00, 82614856.90it/s]\n",
            "Extracting mydata2/cifar-100-python.tar.gz to mydata2\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=15, lr=0.5, wd=0.0, fig_name='lr=0.5.png', lr_scheduler=False, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIAGRAM FOLDER CREATED /content\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/15.. Learning rate: 0.5000.. Train loss: 4.4533.. Train acc: 0.0289.. Val loss: 4.0963.. Val acc: 0.0552\n",
            "Epoch 2/15.. Learning rate: 0.5000.. Train loss: 3.9675.. Train acc: 0.0727.. Val loss: 3.8273.. Val acc: 0.0950\n",
            "Epoch 3/15.. Learning rate: 0.5000.. Train loss: 3.7004.. Train acc: 0.1156.. Val loss: 3.6096.. Val acc: 0.1347\n",
            "Epoch 4/15.. Learning rate: 0.5000.. Train loss: 3.4839.. Train acc: 0.1474.. Val loss: 3.6284.. Val acc: 0.1543\n",
            "Epoch 5/15.. Learning rate: 0.5000.. Train loss: 3.3004.. Train acc: 0.1842.. Val loss: 3.3053.. Val acc: 0.1916\n",
            "Epoch 6/15.. Learning rate: 0.5000.. Train loss: 3.1312.. Train acc: 0.2149.. Val loss: 3.1962.. Val acc: 0.2110\n",
            "Epoch 7/15.. Learning rate: 0.5000.. Train loss: 2.9585.. Train acc: 0.2469.. Val loss: 3.0180.. Val acc: 0.2431\n",
            "Epoch 8/15.. Learning rate: 0.5000.. Train loss: 2.8180.. Train acc: 0.2758.. Val loss: 2.9131.. Val acc: 0.2688\n",
            "Epoch 9/15.. Learning rate: 0.5000.. Train loss: 2.6509.. Train acc: 0.3081.. Val loss: 2.8758.. Val acc: 0.2713\n",
            "Epoch 10/15.. Learning rate: 0.5000.. Train loss: 2.5245.. Train acc: 0.3337.. Val loss: 2.6768.. Val acc: 0.3167\n",
            "Epoch 11/15.. Learning rate: 0.5000.. Train loss: 2.4006.. Train acc: 0.3591.. Val loss: 2.5238.. Val acc: 0.3480\n",
            "Epoch 12/15.. Learning rate: 0.5000.. Train loss: 2.2955.. Train acc: 0.3807.. Val loss: 2.3838.. Val acc: 0.3762\n",
            "Epoch 13/15.. Learning rate: 0.5000.. Train loss: 2.2037.. Train acc: 0.4023.. Val loss: 2.4531.. Val acc: 0.3701\n",
            "Epoch 14/15.. Learning rate: 0.5000.. Train loss: 2.1196.. Train acc: 0.4231.. Val loss: 2.3668.. Val acc: 0.3893\n",
            "Epoch 15/15.. Learning rate: 0.5000.. Train loss: 2.0589.. Train acc: 0.4380.. Val loss: 2.1853.. Val acc: 0.4187\n",
            "Image to be saved: lr=0.558.png\n",
            "Test loss:  2.1708181556701662\n",
            "Test acc:  0.424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H9UaoCEHkiwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 15 \\\n",
        "--lr 0.05 --wd 0.0\\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.05.png \\\n",
        "--test"
      ],
      "metadata": {
        "id": "Aq2z9qCqm6gV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc44723c-f510-48d2-f15c-963bae7db667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=15, lr=0.05, wd=0.0, fig_name='lr=0.05.png', lr_scheduler=False, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIR ----> DIAGRAM not created ---->[Errno 17] File exists: '/content/diagram'\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/15.. Learning rate: 0.0500.. Train loss: 4.1541.. Train acc: 0.0624.. Val loss: 3.8012.. Val acc: 0.1021\n",
            "Epoch 2/15.. Learning rate: 0.0500.. Train loss: 3.6344.. Train acc: 0.1340.. Val loss: 3.4256.. Val acc: 0.1598\n",
            "Epoch 3/15.. Learning rate: 0.0500.. Train loss: 3.3309.. Train acc: 0.1830.. Val loss: 3.1975.. Val acc: 0.2030\n",
            "Epoch 4/15.. Learning rate: 0.0500.. Train loss: 3.1024.. Train acc: 0.2254.. Val loss: 2.9745.. Val acc: 0.2518\n",
            "Epoch 5/15.. Learning rate: 0.0500.. Train loss: 2.8940.. Train acc: 0.2651.. Val loss: 2.8330.. Val acc: 0.2801\n",
            "Epoch 6/15.. Learning rate: 0.0500.. Train loss: 2.7012.. Train acc: 0.3044.. Val loss: 2.6391.. Val acc: 0.3150\n",
            "Epoch 7/15.. Learning rate: 0.0500.. Train loss: 2.5282.. Train acc: 0.3359.. Val loss: 2.5895.. Val acc: 0.3227\n",
            "Epoch 8/15.. Learning rate: 0.0500.. Train loss: 2.3817.. Train acc: 0.3679.. Val loss: 2.4227.. Val acc: 0.3601\n",
            "Epoch 9/15.. Learning rate: 0.0500.. Train loss: 2.2434.. Train acc: 0.3993.. Val loss: 2.4185.. Val acc: 0.3682\n",
            "Epoch 10/15.. Learning rate: 0.0500.. Train loss: 2.1415.. Train acc: 0.4184.. Val loss: 2.1915.. Val acc: 0.4131\n",
            "Epoch 11/15.. Learning rate: 0.0500.. Train loss: 2.0240.. Train acc: 0.4433.. Val loss: 2.1421.. Val acc: 0.4295\n",
            "Epoch 12/15.. Learning rate: 0.0500.. Train loss: 1.9339.. Train acc: 0.4648.. Val loss: 2.0741.. Val acc: 0.4482\n",
            "Epoch 13/15.. Learning rate: 0.0500.. Train loss: 1.8470.. Train acc: 0.4853.. Val loss: 2.1456.. Val acc: 0.4313\n",
            "Epoch 14/15.. Learning rate: 0.0500.. Train loss: 1.7738.. Train acc: 0.5039.. Val loss: 2.0009.. Val acc: 0.4579\n",
            "Epoch 15/15.. Learning rate: 0.0500.. Train loss: 1.6994.. Train acc: 0.5189.. Val loss: 1.9714.. Val acc: 0.4750\n",
            "Image to be saved: lr=0.0560.png\n",
            "Test loss:  1.967647466278076\n",
            "Test acc:  0.472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr = 0.01"
      ],
      "metadata": {
        "id": "33VM2p5TXQaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UL8e8gH_h44d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 15 \\\n",
        "--lr 0.01 --wd 0.0\\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.01.png \\\n",
        "--test"
      ],
      "metadata": {
        "id": "EBIWWGPIXCPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7492b46-2076-40d7-e111-91086c4a22fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=15, lr=0.01, wd=0.0, fig_name='lr=0.01.png', lr_scheduler=False, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIR ----> DIAGRAM not created ---->[Errno 17] File exists: '/content/diagram'\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/15.. Learning rate: 0.0100.. Train loss: 4.3152.. Train acc: 0.0428.. Val loss: 3.9530.. Val acc: 0.0840\n",
            "Epoch 2/15.. Learning rate: 0.0100.. Train loss: 3.7900.. Train acc: 0.1108.. Val loss: 3.5628.. Val acc: 0.1495\n",
            "Epoch 3/15.. Learning rate: 0.0100.. Train loss: 3.4817.. Train acc: 0.1606.. Val loss: 3.3768.. Val acc: 0.1813\n",
            "Epoch 4/15.. Learning rate: 0.0100.. Train loss: 3.2604.. Train acc: 0.1970.. Val loss: 3.1399.. Val acc: 0.2276\n",
            "Epoch 5/15.. Learning rate: 0.0100.. Train loss: 3.0876.. Train acc: 0.2314.. Val loss: 3.0051.. Val acc: 0.2538\n",
            "Epoch 6/15.. Learning rate: 0.0100.. Train loss: 2.9435.. Train acc: 0.2567.. Val loss: 2.8992.. Val acc: 0.2680\n",
            "Epoch 7/15.. Learning rate: 0.0100.. Train loss: 2.8141.. Train acc: 0.2828.. Val loss: 2.8196.. Val acc: 0.2876\n",
            "Epoch 8/15.. Learning rate: 0.0100.. Train loss: 2.6825.. Train acc: 0.3105.. Val loss: 2.6985.. Val acc: 0.3159\n",
            "Epoch 9/15.. Learning rate: 0.0100.. Train loss: 2.5623.. Train acc: 0.3375.. Val loss: 2.6248.. Val acc: 0.3313\n",
            "Epoch 10/15.. Learning rate: 0.0100.. Train loss: 2.4558.. Train acc: 0.3556.. Val loss: 2.5641.. Val acc: 0.3443\n",
            "Epoch 11/15.. Learning rate: 0.0100.. Train loss: 2.3334.. Train acc: 0.3821.. Val loss: 2.4827.. Val acc: 0.3594\n",
            "Epoch 12/15.. Learning rate: 0.0100.. Train loss: 2.2401.. Train acc: 0.4013.. Val loss: 2.3603.. Val acc: 0.3847\n",
            "Epoch 13/15.. Learning rate: 0.0100.. Train loss: 2.1400.. Train acc: 0.4205.. Val loss: 2.3137.. Val acc: 0.3981\n",
            "Epoch 14/15.. Learning rate: 0.0100.. Train loss: 2.0599.. Train acc: 0.4421.. Val loss: 2.2761.. Val acc: 0.4040\n",
            "Epoch 15/15.. Learning rate: 0.0100.. Train loss: 1.9801.. Train acc: 0.4582.. Val loss: 2.2213.. Val acc: 0.4225\n",
            "Image to be saved: lr=0.0130.png\n",
            "Test loss:  2.189468633079529\n",
            "Test acc:  0.4276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LEARNING RATE SCHEDULE"
      ],
      "metadata": {
        "id": "ZXyR1euBT5wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr 0.05 with 300 epochs"
      ],
      "metadata": {
        "id": "-wX4RKhBPvi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 300 \\\n",
        "--lr 0.05 --wd 0.0\\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.05_epochs=300_lr_sch.png \\\n",
        "--test"
      ],
      "metadata": {
        "id": "4ec5H9OKP2al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f18217d-4e71-4351-b4d6-26b3849011cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=300, lr=0.05, wd=0.0, fig_name='lr=0.05_epochs=300.png', lr_scheduler=False, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIR ----> DIAGRAM not created ---->[Errno 17] File exists: '/content/diagram'\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1692.. Train acc: 0.0630.. Val loss: 3.7816.. Val acc: 0.1162\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6320.. Train acc: 0.1310.. Val loss: 3.6196.. Val acc: 0.1482\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.3332.. Train acc: 0.1822.. Val loss: 3.2295.. Val acc: 0.1978\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.0992.. Train acc: 0.2273.. Val loss: 3.0924.. Val acc: 0.2397\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.8794.. Train acc: 0.2657.. Val loss: 2.8530.. Val acc: 0.2813\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.6775.. Train acc: 0.3085.. Val loss: 2.6492.. Val acc: 0.3191\n",
            "Epoch 7/300.. Learning rate: 0.0500.. Train loss: 2.5100.. Train acc: 0.3385.. Val loss: 2.4947.. Val acc: 0.3486\n",
            "Epoch 8/300.. Learning rate: 0.0500.. Train loss: 2.3815.. Train acc: 0.3639.. Val loss: 2.3699.. Val acc: 0.3711\n",
            "Epoch 9/300.. Learning rate: 0.0500.. Train loss: 2.2436.. Train acc: 0.3981.. Val loss: 2.3262.. Val acc: 0.3894\n",
            "Epoch 10/300.. Learning rate: 0.0500.. Train loss: 2.1358.. Train acc: 0.4205.. Val loss: 2.3312.. Val acc: 0.3927\n",
            "Epoch 11/300.. Learning rate: 0.0500.. Train loss: 2.0354.. Train acc: 0.4379.. Val loss: 2.2153.. Val acc: 0.4233\n",
            "Epoch 12/300.. Learning rate: 0.0500.. Train loss: 1.9427.. Train acc: 0.4636.. Val loss: 2.1377.. Val acc: 0.4351\n",
            "Epoch 13/300.. Learning rate: 0.0500.. Train loss: 1.8516.. Train acc: 0.4854.. Val loss: 2.0415.. Val acc: 0.4504\n",
            "Epoch 14/300.. Learning rate: 0.0500.. Train loss: 1.7741.. Train acc: 0.5037.. Val loss: 2.0321.. Val acc: 0.4669\n",
            "Epoch 15/300.. Learning rate: 0.0500.. Train loss: 1.7056.. Train acc: 0.5184.. Val loss: 2.0696.. Val acc: 0.4607\n",
            "Epoch 16/300.. Learning rate: 0.0500.. Train loss: 1.6408.. Train acc: 0.5310.. Val loss: 2.0130.. Val acc: 0.4736\n",
            "Epoch 17/300.. Learning rate: 0.0500.. Train loss: 1.5733.. Train acc: 0.5492.. Val loss: 1.9550.. Val acc: 0.4870\n",
            "Epoch 18/300.. Learning rate: 0.0500.. Train loss: 1.5122.. Train acc: 0.5610.. Val loss: 1.9317.. Val acc: 0.4887\n",
            "Epoch 19/300.. Learning rate: 0.0500.. Train loss: 1.4507.. Train acc: 0.5795.. Val loss: 1.9934.. Val acc: 0.4817\n",
            "Epoch 20/300.. Learning rate: 0.0500.. Train loss: 1.3845.. Train acc: 0.5935.. Val loss: 1.9424.. Val acc: 0.4966\n",
            "Epoch 21/300.. Learning rate: 0.0500.. Train loss: 1.3366.. Train acc: 0.6051.. Val loss: 1.9016.. Val acc: 0.5057\n",
            "Epoch 22/300.. Learning rate: 0.0500.. Train loss: 1.2836.. Train acc: 0.6193.. Val loss: 1.9105.. Val acc: 0.5040\n",
            "Epoch 23/300.. Learning rate: 0.0500.. Train loss: 1.2233.. Train acc: 0.6376.. Val loss: 1.8542.. Val acc: 0.5220\n",
            "Epoch 24/300.. Learning rate: 0.0500.. Train loss: 1.1743.. Train acc: 0.6475.. Val loss: 1.9499.. Val acc: 0.5169\n",
            "Epoch 25/300.. Learning rate: 0.0500.. Train loss: 1.1271.. Train acc: 0.6598.. Val loss: 1.9397.. Val acc: 0.5144\n",
            "Epoch 26/300.. Learning rate: 0.0500.. Train loss: 1.0839.. Train acc: 0.6718.. Val loss: 1.9206.. Val acc: 0.5230\n",
            "Epoch 27/300.. Learning rate: 0.0500.. Train loss: 1.0310.. Train acc: 0.6858.. Val loss: 1.9491.. Val acc: 0.5205\n",
            "Epoch 28/300.. Learning rate: 0.0500.. Train loss: 0.9817.. Train acc: 0.6984.. Val loss: 1.9500.. Val acc: 0.5182\n",
            "Epoch 29/300.. Learning rate: 0.0500.. Train loss: 0.9372.. Train acc: 0.7109.. Val loss: 1.9808.. Val acc: 0.5234\n",
            "Epoch 30/300.. Learning rate: 0.0500.. Train loss: 0.8969.. Train acc: 0.7208.. Val loss: 2.0117.. Val acc: 0.5248\n",
            "Epoch 31/300.. Learning rate: 0.0500.. Train loss: 0.8405.. Train acc: 0.7371.. Val loss: 1.9822.. Val acc: 0.5299\n",
            "Epoch 32/300.. Learning rate: 0.0500.. Train loss: 0.8053.. Train acc: 0.7466.. Val loss: 2.0708.. Val acc: 0.5256\n",
            "Epoch 33/300.. Learning rate: 0.0500.. Train loss: 0.7522.. Train acc: 0.7612.. Val loss: 2.0408.. Val acc: 0.5414\n",
            "Epoch 34/300.. Learning rate: 0.0500.. Train loss: 0.7081.. Train acc: 0.7724.. Val loss: 2.0920.. Val acc: 0.5319\n",
            "Epoch 35/300.. Learning rate: 0.0500.. Train loss: 0.6742.. Train acc: 0.7827.. Val loss: 2.1671.. Val acc: 0.5236\n",
            "Epoch 36/300.. Learning rate: 0.0500.. Train loss: 0.6324.. Train acc: 0.7958.. Val loss: 2.1527.. Val acc: 0.5380\n",
            "Epoch 37/300.. Learning rate: 0.0500.. Train loss: 0.5910.. Train acc: 0.8087.. Val loss: 2.2121.. Val acc: 0.5313\n",
            "Epoch 38/300.. Learning rate: 0.0500.. Train loss: 0.5685.. Train acc: 0.8170.. Val loss: 2.2419.. Val acc: 0.5261\n",
            "Epoch 39/300.. Learning rate: 0.0500.. Train loss: 0.5303.. Train acc: 0.8273.. Val loss: 2.2033.. Val acc: 0.5331\n",
            "Epoch 40/300.. Learning rate: 0.0500.. Train loss: 0.4909.. Train acc: 0.8390.. Val loss: 2.2840.. Val acc: 0.5376\n",
            "Epoch 41/300.. Learning rate: 0.0500.. Train loss: 0.4757.. Train acc: 0.8470.. Val loss: 2.3554.. Val acc: 0.5313\n",
            "Epoch 42/300.. Learning rate: 0.0500.. Train loss: 0.4441.. Train acc: 0.8554.. Val loss: 2.4141.. Val acc: 0.5219\n",
            "Epoch 43/300.. Learning rate: 0.0500.. Train loss: 0.4159.. Train acc: 0.8639.. Val loss: 2.3614.. Val acc: 0.5368\n",
            "Epoch 44/300.. Learning rate: 0.0500.. Train loss: 0.3815.. Train acc: 0.8748.. Val loss: 2.2956.. Val acc: 0.5470\n",
            "Epoch 45/300.. Learning rate: 0.0500.. Train loss: 0.3632.. Train acc: 0.8811.. Val loss: 2.4047.. Val acc: 0.5338\n",
            "Epoch 46/300.. Learning rate: 0.0500.. Train loss: 0.3297.. Train acc: 0.8908.. Val loss: 2.4078.. Val acc: 0.5406\n",
            "Epoch 47/300.. Learning rate: 0.0500.. Train loss: 0.3300.. Train acc: 0.8920.. Val loss: 2.4879.. Val acc: 0.5321\n",
            "Epoch 48/300.. Learning rate: 0.0500.. Train loss: 0.3058.. Train acc: 0.9001.. Val loss: 2.4331.. Val acc: 0.5452\n",
            "Epoch 49/300.. Learning rate: 0.0500.. Train loss: 0.2795.. Train acc: 0.9081.. Val loss: 2.4682.. Val acc: 0.5446\n",
            "Epoch 50/300.. Learning rate: 0.0500.. Train loss: 0.2663.. Train acc: 0.9129.. Val loss: 2.4791.. Val acc: 0.5516\n",
            "Epoch 51/300.. Learning rate: 0.0500.. Train loss: 0.2533.. Train acc: 0.9180.. Val loss: 2.5171.. Val acc: 0.5465\n",
            "Epoch 52/300.. Learning rate: 0.0500.. Train loss: 0.2481.. Train acc: 0.9191.. Val loss: 2.5121.. Val acc: 0.5486\n",
            "Epoch 53/300.. Learning rate: 0.0500.. Train loss: 0.2361.. Train acc: 0.9222.. Val loss: 2.5772.. Val acc: 0.5493\n",
            "Epoch 54/300.. Learning rate: 0.0500.. Train loss: 0.2219.. Train acc: 0.9261.. Val loss: 2.5954.. Val acc: 0.5471\n",
            "Epoch 55/300.. Learning rate: 0.0500.. Train loss: 0.2180.. Train acc: 0.9292.. Val loss: 2.6876.. Val acc: 0.5383\n",
            "Epoch 56/300.. Learning rate: 0.0500.. Train loss: 0.2088.. Train acc: 0.9308.. Val loss: 2.6217.. Val acc: 0.5513\n",
            "Epoch 57/300.. Learning rate: 0.0500.. Train loss: 0.1981.. Train acc: 0.9346.. Val loss: 2.6843.. Val acc: 0.5452\n",
            "Epoch 58/300.. Learning rate: 0.0500.. Train loss: 0.1888.. Train acc: 0.9392.. Val loss: 2.6543.. Val acc: 0.5421\n",
            "Epoch 59/300.. Learning rate: 0.0500.. Train loss: 0.1749.. Train acc: 0.9426.. Val loss: 2.6872.. Val acc: 0.5499\n",
            "Epoch 60/300.. Learning rate: 0.0500.. Train loss: 0.1680.. Train acc: 0.9450.. Val loss: 2.6897.. Val acc: 0.5490\n",
            "Epoch 61/300.. Learning rate: 0.0500.. Train loss: 0.1554.. Train acc: 0.9489.. Val loss: 2.6557.. Val acc: 0.5487\n",
            "Epoch 62/300.. Learning rate: 0.0500.. Train loss: 0.1434.. Train acc: 0.9538.. Val loss: 2.6788.. Val acc: 0.5510\n",
            "Epoch 63/300.. Learning rate: 0.0500.. Train loss: 0.1492.. Train acc: 0.9504.. Val loss: 2.7425.. Val acc: 0.5500\n",
            "Epoch 64/300.. Learning rate: 0.0500.. Train loss: 0.1400.. Train acc: 0.9553.. Val loss: 2.6716.. Val acc: 0.5527\n",
            "Epoch 65/300.. Learning rate: 0.0500.. Train loss: 0.1347.. Train acc: 0.9569.. Val loss: 2.7095.. Val acc: 0.5572\n",
            "Epoch 66/300.. Learning rate: 0.0500.. Train loss: 0.1391.. Train acc: 0.9553.. Val loss: 2.7813.. Val acc: 0.5462\n",
            "Epoch 67/300.. Learning rate: 0.0500.. Train loss: 0.1133.. Train acc: 0.9633.. Val loss: 2.7400.. Val acc: 0.5517\n",
            "Epoch 68/300.. Learning rate: 0.0500.. Train loss: 0.1134.. Train acc: 0.9636.. Val loss: 2.7733.. Val acc: 0.5508\n",
            "Epoch 69/300.. Learning rate: 0.0500.. Train loss: 0.1097.. Train acc: 0.9639.. Val loss: 2.7890.. Val acc: 0.5542\n",
            "Epoch 70/300.. Learning rate: 0.0500.. Train loss: 0.1036.. Train acc: 0.9663.. Val loss: 2.7982.. Val acc: 0.5532\n",
            "Epoch 71/300.. Learning rate: 0.0500.. Train loss: 0.0971.. Train acc: 0.9681.. Val loss: 2.8326.. Val acc: 0.5529\n",
            "Epoch 72/300.. Learning rate: 0.0500.. Train loss: 0.1037.. Train acc: 0.9655.. Val loss: 2.8234.. Val acc: 0.5556\n",
            "Epoch 73/300.. Learning rate: 0.0500.. Train loss: 0.1004.. Train acc: 0.9673.. Val loss: 2.8778.. Val acc: 0.5476\n",
            "Epoch 74/300.. Learning rate: 0.0500.. Train loss: 0.0998.. Train acc: 0.9676.. Val loss: 2.8833.. Val acc: 0.5533\n",
            "Epoch 75/300.. Learning rate: 0.0500.. Train loss: 0.0961.. Train acc: 0.9689.. Val loss: 2.8771.. Val acc: 0.5546\n",
            "Epoch 76/300.. Learning rate: 0.0500.. Train loss: 0.0868.. Train acc: 0.9718.. Val loss: 2.8425.. Val acc: 0.5567\n",
            "Epoch 77/300.. Learning rate: 0.0500.. Train loss: 0.0790.. Train acc: 0.9745.. Val loss: 2.8570.. Val acc: 0.5551\n",
            "Epoch 78/300.. Learning rate: 0.0500.. Train loss: 0.0822.. Train acc: 0.9746.. Val loss: 2.9716.. Val acc: 0.5521\n",
            "Epoch 79/300.. Learning rate: 0.0500.. Train loss: 0.0839.. Train acc: 0.9737.. Val loss: 2.9231.. Val acc: 0.5556\n",
            "Epoch 80/300.. Learning rate: 0.0500.. Train loss: 0.0784.. Train acc: 0.9750.. Val loss: 2.9368.. Val acc: 0.5563\n",
            "Epoch 81/300.. Learning rate: 0.0500.. Train loss: 0.0774.. Train acc: 0.9749.. Val loss: 2.9629.. Val acc: 0.5547\n",
            "Epoch 82/300.. Learning rate: 0.0500.. Train loss: 0.0729.. Train acc: 0.9773.. Val loss: 2.9667.. Val acc: 0.5562\n",
            "Epoch 83/300.. Learning rate: 0.0500.. Train loss: 0.0625.. Train acc: 0.9796.. Val loss: 2.9992.. Val acc: 0.5605\n",
            "Epoch 84/300.. Learning rate: 0.0500.. Train loss: 0.0627.. Train acc: 0.9805.. Val loss: 2.9791.. Val acc: 0.5634\n",
            "Epoch 85/300.. Learning rate: 0.0500.. Train loss: 0.0634.. Train acc: 0.9806.. Val loss: 3.0047.. Val acc: 0.5632\n",
            "Epoch 86/300.. Learning rate: 0.0500.. Train loss: 0.0652.. Train acc: 0.9790.. Val loss: 2.9751.. Val acc: 0.5665\n",
            "Epoch 87/300.. Learning rate: 0.0500.. Train loss: 0.0696.. Train acc: 0.9772.. Val loss: 2.9919.. Val acc: 0.5608\n",
            "Epoch 88/300.. Learning rate: 0.0500.. Train loss: 0.0633.. Train acc: 0.9802.. Val loss: 3.0545.. Val acc: 0.5566\n",
            "Epoch 89/300.. Learning rate: 0.0500.. Train loss: 0.0612.. Train acc: 0.9807.. Val loss: 2.9984.. Val acc: 0.5582\n",
            "Epoch 90/300.. Learning rate: 0.0500.. Train loss: 0.0617.. Train acc: 0.9800.. Val loss: 3.0348.. Val acc: 0.5588\n",
            "Epoch 91/300.. Learning rate: 0.0500.. Train loss: 0.0615.. Train acc: 0.9795.. Val loss: 3.0666.. Val acc: 0.5524\n",
            "Epoch 92/300.. Learning rate: 0.0500.. Train loss: 0.0591.. Train acc: 0.9809.. Val loss: 3.0414.. Val acc: 0.5601\n",
            "Epoch 93/300.. Learning rate: 0.0500.. Train loss: 0.0576.. Train acc: 0.9815.. Val loss: 3.0544.. Val acc: 0.5644\n",
            "Epoch 94/300.. Learning rate: 0.0500.. Train loss: 0.0565.. Train acc: 0.9814.. Val loss: 3.0119.. Val acc: 0.5632\n",
            "Epoch 95/300.. Learning rate: 0.0500.. Train loss: 0.0525.. Train acc: 0.9838.. Val loss: 3.0480.. Val acc: 0.5675\n",
            "Epoch 96/300.. Learning rate: 0.0500.. Train loss: 0.0536.. Train acc: 0.9828.. Val loss: 3.0440.. Val acc: 0.5631\n",
            "Epoch 97/300.. Learning rate: 0.0500.. Train loss: 0.0471.. Train acc: 0.9843.. Val loss: 3.0234.. Val acc: 0.5693\n",
            "Epoch 98/300.. Learning rate: 0.0500.. Train loss: 0.0466.. Train acc: 0.9852.. Val loss: 3.0776.. Val acc: 0.5606\n",
            "Epoch 99/300.. Learning rate: 0.0500.. Train loss: 0.0408.. Train acc: 0.9875.. Val loss: 3.0987.. Val acc: 0.5663\n",
            "Epoch 100/300.. Learning rate: 0.0500.. Train loss: 0.0524.. Train acc: 0.9835.. Val loss: 3.0957.. Val acc: 0.5542\n",
            "Epoch 101/300.. Learning rate: 0.0500.. Train loss: 0.0452.. Train acc: 0.9859.. Val loss: 3.0882.. Val acc: 0.5599\n",
            "Epoch 102/300.. Learning rate: 0.0500.. Train loss: 0.0481.. Train acc: 0.9851.. Val loss: 3.1167.. Val acc: 0.5597\n",
            "Epoch 103/300.. Learning rate: 0.0500.. Train loss: 0.0515.. Train acc: 0.9839.. Val loss: 3.1407.. Val acc: 0.5571\n",
            "Epoch 104/300.. Learning rate: 0.0500.. Train loss: 0.0477.. Train acc: 0.9849.. Val loss: 3.1548.. Val acc: 0.5586\n",
            "Epoch 105/300.. Learning rate: 0.0500.. Train loss: 0.0491.. Train acc: 0.9847.. Val loss: 3.1325.. Val acc: 0.5626\n",
            "Epoch 106/300.. Learning rate: 0.0500.. Train loss: 0.0467.. Train acc: 0.9858.. Val loss: 3.1168.. Val acc: 0.5615\n",
            "Epoch 107/300.. Learning rate: 0.0500.. Train loss: 0.0441.. Train acc: 0.9860.. Val loss: 3.1316.. Val acc: 0.5641\n",
            "Epoch 108/300.. Learning rate: 0.0500.. Train loss: 0.0412.. Train acc: 0.9867.. Val loss: 3.1575.. Val acc: 0.5667\n",
            "Epoch 109/300.. Learning rate: 0.0500.. Train loss: 0.0407.. Train acc: 0.9870.. Val loss: 3.1244.. Val acc: 0.5662\n",
            "Epoch 110/300.. Learning rate: 0.0500.. Train loss: 0.0423.. Train acc: 0.9863.. Val loss: 3.1556.. Val acc: 0.5652\n",
            "Epoch 111/300.. Learning rate: 0.0500.. Train loss: 0.0395.. Train acc: 0.9876.. Val loss: 3.1272.. Val acc: 0.5630\n",
            "Epoch 112/300.. Learning rate: 0.0500.. Train loss: 0.0398.. Train acc: 0.9871.. Val loss: 3.1553.. Val acc: 0.5652\n",
            "Epoch 113/300.. Learning rate: 0.0500.. Train loss: 0.0361.. Train acc: 0.9882.. Val loss: 3.1143.. Val acc: 0.5686\n",
            "Epoch 114/300.. Learning rate: 0.0500.. Train loss: 0.0356.. Train acc: 0.9891.. Val loss: 3.1397.. Val acc: 0.5642\n",
            "Epoch 115/300.. Learning rate: 0.0500.. Train loss: 0.0361.. Train acc: 0.9882.. Val loss: 3.1529.. Val acc: 0.5668\n",
            "Epoch 116/300.. Learning rate: 0.0500.. Train loss: 0.0302.. Train acc: 0.9908.. Val loss: 3.1580.. Val acc: 0.5659\n",
            "Epoch 117/300.. Learning rate: 0.0500.. Train loss: 0.0349.. Train acc: 0.9891.. Val loss: 3.1585.. Val acc: 0.5661\n",
            "Epoch 118/300.. Learning rate: 0.0500.. Train loss: 0.0310.. Train acc: 0.9904.. Val loss: 3.1904.. Val acc: 0.5651\n",
            "Epoch 119/300.. Learning rate: 0.0500.. Train loss: 0.0318.. Train acc: 0.9901.. Val loss: 3.1921.. Val acc: 0.5653\n",
            "Epoch 120/300.. Learning rate: 0.0500.. Train loss: 0.0299.. Train acc: 0.9905.. Val loss: 3.1697.. Val acc: 0.5655\n",
            "Epoch 121/300.. Learning rate: 0.0500.. Train loss: 0.0284.. Train acc: 0.9912.. Val loss: 3.2584.. Val acc: 0.5649\n",
            "Epoch 122/300.. Learning rate: 0.0500.. Train loss: 0.0286.. Train acc: 0.9917.. Val loss: 3.2121.. Val acc: 0.5652\n",
            "Epoch 123/300.. Learning rate: 0.0500.. Train loss: 0.0288.. Train acc: 0.9909.. Val loss: 3.2099.. Val acc: 0.5647\n",
            "Epoch 124/300.. Learning rate: 0.0500.. Train loss: 0.0308.. Train acc: 0.9905.. Val loss: 3.2025.. Val acc: 0.5672\n",
            "Epoch 125/300.. Learning rate: 0.0500.. Train loss: 0.0275.. Train acc: 0.9913.. Val loss: 3.2278.. Val acc: 0.5646\n",
            "Epoch 126/300.. Learning rate: 0.0500.. Train loss: 0.0269.. Train acc: 0.9917.. Val loss: 3.2300.. Val acc: 0.5648\n",
            "Epoch 127/300.. Learning rate: 0.0500.. Train loss: 0.0285.. Train acc: 0.9911.. Val loss: 3.2392.. Val acc: 0.5601\n",
            "Epoch 128/300.. Learning rate: 0.0500.. Train loss: 0.0265.. Train acc: 0.9919.. Val loss: 3.2490.. Val acc: 0.5600\n",
            "Epoch 129/300.. Learning rate: 0.0500.. Train loss: 0.0276.. Train acc: 0.9911.. Val loss: 3.2443.. Val acc: 0.5603\n",
            "Epoch 130/300.. Learning rate: 0.0500.. Train loss: 0.0279.. Train acc: 0.9913.. Val loss: 3.2332.. Val acc: 0.5579\n",
            "Epoch 131/300.. Learning rate: 0.0500.. Train loss: 0.0317.. Train acc: 0.9897.. Val loss: 3.2586.. Val acc: 0.5557\n",
            "Epoch 132/300.. Learning rate: 0.0500.. Train loss: 0.0275.. Train acc: 0.9911.. Val loss: 3.2652.. Val acc: 0.5617\n",
            "Epoch 133/300.. Learning rate: 0.0500.. Train loss: 0.0257.. Train acc: 0.9916.. Val loss: 3.2418.. Val acc: 0.5660\n",
            "Epoch 134/300.. Learning rate: 0.0500.. Train loss: 0.0255.. Train acc: 0.9925.. Val loss: 3.3029.. Val acc: 0.5622\n",
            "Epoch 135/300.. Learning rate: 0.0500.. Train loss: 0.0269.. Train acc: 0.9912.. Val loss: 3.2874.. Val acc: 0.5610\n",
            "Epoch 136/300.. Learning rate: 0.0500.. Train loss: 0.0247.. Train acc: 0.9923.. Val loss: 3.2631.. Val acc: 0.5668\n",
            "Epoch 137/300.. Learning rate: 0.0500.. Train loss: 0.0268.. Train acc: 0.9916.. Val loss: 3.3042.. Val acc: 0.5637\n",
            "Epoch 138/300.. Learning rate: 0.0500.. Train loss: 0.0248.. Train acc: 0.9922.. Val loss: 3.2435.. Val acc: 0.5673\n",
            "Epoch 139/300.. Learning rate: 0.0500.. Train loss: 0.0262.. Train acc: 0.9922.. Val loss: 3.2604.. Val acc: 0.5688\n",
            "Epoch 140/300.. Learning rate: 0.0500.. Train loss: 0.0230.. Train acc: 0.9922.. Val loss: 3.3032.. Val acc: 0.5623\n",
            "Epoch 141/300.. Learning rate: 0.0500.. Train loss: 0.0250.. Train acc: 0.9912.. Val loss: 3.2912.. Val acc: 0.5657\n",
            "Epoch 142/300.. Learning rate: 0.0500.. Train loss: 0.0252.. Train acc: 0.9915.. Val loss: 3.2829.. Val acc: 0.5669\n",
            "Epoch 143/300.. Learning rate: 0.0500.. Train loss: 0.0219.. Train acc: 0.9935.. Val loss: 3.2959.. Val acc: 0.5657\n",
            "Epoch 144/300.. Learning rate: 0.0500.. Train loss: 0.0265.. Train acc: 0.9912.. Val loss: 3.2920.. Val acc: 0.5672\n",
            "Epoch 145/300.. Learning rate: 0.0500.. Train loss: 0.0283.. Train acc: 0.9902.. Val loss: 3.3239.. Val acc: 0.5641\n",
            "Epoch 146/300.. Learning rate: 0.0500.. Train loss: 0.0278.. Train acc: 0.9909.. Val loss: 3.3356.. Val acc: 0.5659\n",
            "Epoch 147/300.. Learning rate: 0.0500.. Train loss: 0.0254.. Train acc: 0.9923.. Val loss: 3.3086.. Val acc: 0.5653\n",
            "Epoch 148/300.. Learning rate: 0.0500.. Train loss: 0.0249.. Train acc: 0.9922.. Val loss: 3.3355.. Val acc: 0.5653\n",
            "Epoch 149/300.. Learning rate: 0.0500.. Train loss: 0.0242.. Train acc: 0.9922.. Val loss: 3.3209.. Val acc: 0.5625\n",
            "Epoch 150/300.. Learning rate: 0.0500.. Train loss: 0.0232.. Train acc: 0.9927.. Val loss: 3.2528.. Val acc: 0.5657\n",
            "Epoch 151/300.. Learning rate: 0.0500.. Train loss: 0.0196.. Train acc: 0.9937.. Val loss: 3.2813.. Val acc: 0.5660\n",
            "Epoch 152/300.. Learning rate: 0.0500.. Train loss: 0.0189.. Train acc: 0.9944.. Val loss: 3.2447.. Val acc: 0.5626\n",
            "Epoch 153/300.. Learning rate: 0.0500.. Train loss: 0.0212.. Train acc: 0.9928.. Val loss: 3.2671.. Val acc: 0.5704\n",
            "Epoch 154/300.. Learning rate: 0.0500.. Train loss: 0.0224.. Train acc: 0.9929.. Val loss: 3.3035.. Val acc: 0.5671\n",
            "Epoch 155/300.. Learning rate: 0.0500.. Train loss: 0.0184.. Train acc: 0.9944.. Val loss: 3.3044.. Val acc: 0.5678\n",
            "Epoch 156/300.. Learning rate: 0.0500.. Train loss: 0.0195.. Train acc: 0.9937.. Val loss: 3.3318.. Val acc: 0.5717\n",
            "Epoch 157/300.. Learning rate: 0.0500.. Train loss: 0.0153.. Train acc: 0.9955.. Val loss: 3.2926.. Val acc: 0.5728\n",
            "Epoch 158/300.. Learning rate: 0.0500.. Train loss: 0.0172.. Train acc: 0.9948.. Val loss: 3.3182.. Val acc: 0.5726\n",
            "Epoch 159/300.. Learning rate: 0.0500.. Train loss: 0.0179.. Train acc: 0.9946.. Val loss: 3.3105.. Val acc: 0.5710\n",
            "Epoch 160/300.. Learning rate: 0.0500.. Train loss: 0.0190.. Train acc: 0.9943.. Val loss: 3.3592.. Val acc: 0.5664\n",
            "Epoch 161/300.. Learning rate: 0.0500.. Train loss: 0.0200.. Train acc: 0.9933.. Val loss: 3.3706.. Val acc: 0.5675\n",
            "Epoch 162/300.. Learning rate: 0.0500.. Train loss: 0.0184.. Train acc: 0.9942.. Val loss: 3.3456.. Val acc: 0.5650\n",
            "Epoch 163/300.. Learning rate: 0.0500.. Train loss: 0.0172.. Train acc: 0.9946.. Val loss: 3.2802.. Val acc: 0.5709\n",
            "Epoch 164/300.. Learning rate: 0.0500.. Train loss: 0.0175.. Train acc: 0.9945.. Val loss: 3.2724.. Val acc: 0.5722\n",
            "Epoch 165/300.. Learning rate: 0.0500.. Train loss: 0.0176.. Train acc: 0.9942.. Val loss: 3.3207.. Val acc: 0.5677\n",
            "Epoch 166/300.. Learning rate: 0.0500.. Train loss: 0.0159.. Train acc: 0.9951.. Val loss: 3.3534.. Val acc: 0.5705\n",
            "Epoch 167/300.. Learning rate: 0.0500.. Train loss: 0.0172.. Train acc: 0.9947.. Val loss: 3.3167.. Val acc: 0.5721\n",
            "Epoch 168/300.. Learning rate: 0.0500.. Train loss: 0.0153.. Train acc: 0.9953.. Val loss: 3.3248.. Val acc: 0.5725\n",
            "Epoch 169/300.. Learning rate: 0.0500.. Train loss: 0.0170.. Train acc: 0.9946.. Val loss: 3.3069.. Val acc: 0.5661\n",
            "Epoch 170/300.. Learning rate: 0.0500.. Train loss: 0.0175.. Train acc: 0.9944.. Val loss: 3.3390.. Val acc: 0.5669\n",
            "Epoch 171/300.. Learning rate: 0.0500.. Train loss: 0.0188.. Train acc: 0.9944.. Val loss: 3.3428.. Val acc: 0.5666\n",
            "Epoch 172/300.. Learning rate: 0.0500.. Train loss: 0.0162.. Train acc: 0.9949.. Val loss: 3.3601.. Val acc: 0.5644\n",
            "Epoch 173/300.. Learning rate: 0.0500.. Train loss: 0.0157.. Train acc: 0.9948.. Val loss: 3.3620.. Val acc: 0.5681\n",
            "Epoch 174/300.. Learning rate: 0.0500.. Train loss: 0.0153.. Train acc: 0.9953.. Val loss: 3.3568.. Val acc: 0.5715\n",
            "Epoch 175/300.. Learning rate: 0.0500.. Train loss: 0.0157.. Train acc: 0.9951.. Val loss: 3.3517.. Val acc: 0.5783\n",
            "Epoch 176/300.. Learning rate: 0.0500.. Train loss: 0.0123.. Train acc: 0.9959.. Val loss: 3.3582.. Val acc: 0.5747\n",
            "Epoch 177/300.. Learning rate: 0.0500.. Train loss: 0.0153.. Train acc: 0.9952.. Val loss: 3.4092.. Val acc: 0.5670\n",
            "Epoch 178/300.. Learning rate: 0.0500.. Train loss: 0.0131.. Train acc: 0.9962.. Val loss: 3.3675.. Val acc: 0.5745\n",
            "Epoch 179/300.. Learning rate: 0.0500.. Train loss: 0.0113.. Train acc: 0.9967.. Val loss: 3.3425.. Val acc: 0.5794\n",
            "Epoch 180/300.. Learning rate: 0.0500.. Train loss: 0.0117.. Train acc: 0.9967.. Val loss: 3.3848.. Val acc: 0.5739\n",
            "Epoch 181/300.. Learning rate: 0.0500.. Train loss: 0.0135.. Train acc: 0.9960.. Val loss: 3.3992.. Val acc: 0.5738\n",
            "Epoch 182/300.. Learning rate: 0.0500.. Train loss: 0.0140.. Train acc: 0.9956.. Val loss: 3.3796.. Val acc: 0.5705\n",
            "Epoch 183/300.. Learning rate: 0.0500.. Train loss: 0.0150.. Train acc: 0.9950.. Val loss: 3.3743.. Val acc: 0.5747\n",
            "Epoch 184/300.. Learning rate: 0.0500.. Train loss: 0.0141.. Train acc: 0.9956.. Val loss: 3.4418.. Val acc: 0.5698\n",
            "Epoch 185/300.. Learning rate: 0.0500.. Train loss: 0.0139.. Train acc: 0.9955.. Val loss: 3.4016.. Val acc: 0.5735\n",
            "Epoch 186/300.. Learning rate: 0.0500.. Train loss: 0.0153.. Train acc: 0.9953.. Val loss: 3.4437.. Val acc: 0.5679\n",
            "Epoch 187/300.. Learning rate: 0.0500.. Train loss: 0.0156.. Train acc: 0.9947.. Val loss: 3.4334.. Val acc: 0.5669\n",
            "Epoch 188/300.. Learning rate: 0.0500.. Train loss: 0.0146.. Train acc: 0.9954.. Val loss: 3.4496.. Val acc: 0.5687\n",
            "Epoch 189/300.. Learning rate: 0.0500.. Train loss: 0.0160.. Train acc: 0.9948.. Val loss: 3.4087.. Val acc: 0.5715\n",
            "Epoch 190/300.. Learning rate: 0.0500.. Train loss: 0.0118.. Train acc: 0.9962.. Val loss: 3.3876.. Val acc: 0.5757\n",
            "Epoch 191/300.. Learning rate: 0.0500.. Train loss: 0.0122.. Train acc: 0.9958.. Val loss: 3.4007.. Val acc: 0.5722\n",
            "Epoch 192/300.. Learning rate: 0.0500.. Train loss: 0.0145.. Train acc: 0.9954.. Val loss: 3.4299.. Val acc: 0.5683\n",
            "Epoch 193/300.. Learning rate: 0.0500.. Train loss: 0.0151.. Train acc: 0.9950.. Val loss: 3.4444.. Val acc: 0.5686\n",
            "Epoch 194/300.. Learning rate: 0.0500.. Train loss: 0.0140.. Train acc: 0.9953.. Val loss: 3.4284.. Val acc: 0.5700\n",
            "Epoch 195/300.. Learning rate: 0.0500.. Train loss: 0.0134.. Train acc: 0.9957.. Val loss: 3.4070.. Val acc: 0.5671\n",
            "Epoch 196/300.. Learning rate: 0.0500.. Train loss: 0.0123.. Train acc: 0.9960.. Val loss: 3.4223.. Val acc: 0.5678\n",
            "Epoch 197/300.. Learning rate: 0.0500.. Train loss: 0.0161.. Train acc: 0.9947.. Val loss: 3.4455.. Val acc: 0.5708\n",
            "Epoch 198/300.. Learning rate: 0.0500.. Train loss: 0.0147.. Train acc: 0.9958.. Val loss: 3.4503.. Val acc: 0.5702\n",
            "Epoch 199/300.. Learning rate: 0.0500.. Train loss: 0.0149.. Train acc: 0.9956.. Val loss: 3.4322.. Val acc: 0.5747\n",
            "Epoch 200/300.. Learning rate: 0.0500.. Train loss: 0.0119.. Train acc: 0.9966.. Val loss: 3.4103.. Val acc: 0.5703\n",
            "Epoch 201/300.. Learning rate: 0.0500.. Train loss: 0.0118.. Train acc: 0.9964.. Val loss: 3.4138.. Val acc: 0.5718\n",
            "Epoch 202/300.. Learning rate: 0.0500.. Train loss: 0.0123.. Train acc: 0.9960.. Val loss: 3.4139.. Val acc: 0.5694\n",
            "Epoch 203/300.. Learning rate: 0.0500.. Train loss: 0.0107.. Train acc: 0.9968.. Val loss: 3.4201.. Val acc: 0.5711\n",
            "Epoch 204/300.. Learning rate: 0.0500.. Train loss: 0.0120.. Train acc: 0.9964.. Val loss: 3.4586.. Val acc: 0.5715\n",
            "Epoch 205/300.. Learning rate: 0.0500.. Train loss: 0.0119.. Train acc: 0.9966.. Val loss: 3.4258.. Val acc: 0.5733\n",
            "Epoch 206/300.. Learning rate: 0.0500.. Train loss: 0.0140.. Train acc: 0.9954.. Val loss: 3.5248.. Val acc: 0.5662\n",
            "Epoch 207/300.. Learning rate: 0.0500.. Train loss: 0.0123.. Train acc: 0.9961.. Val loss: 3.4625.. Val acc: 0.5676\n",
            "Epoch 208/300.. Learning rate: 0.0500.. Train loss: 0.0124.. Train acc: 0.9962.. Val loss: 3.5223.. Val acc: 0.5686\n",
            "Epoch 209/300.. Learning rate: 0.0500.. Train loss: 0.0129.. Train acc: 0.9962.. Val loss: 3.4837.. Val acc: 0.5721\n",
            "Epoch 210/300.. Learning rate: 0.0500.. Train loss: 0.0106.. Train acc: 0.9966.. Val loss: 3.4697.. Val acc: 0.5743\n",
            "Epoch 211/300.. Learning rate: 0.0500.. Train loss: 0.0121.. Train acc: 0.9961.. Val loss: 3.4559.. Val acc: 0.5729\n",
            "Epoch 212/300.. Learning rate: 0.0500.. Train loss: 0.0123.. Train acc: 0.9962.. Val loss: 3.4473.. Val acc: 0.5806\n",
            "Epoch 213/300.. Learning rate: 0.0500.. Train loss: 0.0099.. Train acc: 0.9970.. Val loss: 3.4422.. Val acc: 0.5758\n",
            "Epoch 214/300.. Learning rate: 0.0500.. Train loss: 0.0104.. Train acc: 0.9967.. Val loss: 3.4817.. Val acc: 0.5740\n",
            "Epoch 215/300.. Learning rate: 0.0500.. Train loss: 0.0116.. Train acc: 0.9964.. Val loss: 3.4346.. Val acc: 0.5703\n",
            "Epoch 216/300.. Learning rate: 0.0500.. Train loss: 0.0118.. Train acc: 0.9961.. Val loss: 3.4961.. Val acc: 0.5731\n",
            "Epoch 217/300.. Learning rate: 0.0500.. Train loss: 0.0101.. Train acc: 0.9969.. Val loss: 3.4226.. Val acc: 0.5770\n",
            "Epoch 218/300.. Learning rate: 0.0500.. Train loss: 0.0103.. Train acc: 0.9967.. Val loss: 3.4446.. Val acc: 0.5821\n",
            "Epoch 219/300.. Learning rate: 0.0500.. Train loss: 0.0093.. Train acc: 0.9970.. Val loss: 3.4391.. Val acc: 0.5778\n",
            "Epoch 220/300.. Learning rate: 0.0500.. Train loss: 0.0098.. Train acc: 0.9968.. Val loss: 3.4046.. Val acc: 0.5758\n",
            "Epoch 221/300.. Learning rate: 0.0500.. Train loss: 0.0106.. Train acc: 0.9968.. Val loss: 3.4477.. Val acc: 0.5724\n",
            "Epoch 222/300.. Learning rate: 0.0500.. Train loss: 0.0112.. Train acc: 0.9962.. Val loss: 3.4313.. Val acc: 0.5722\n",
            "Epoch 223/300.. Learning rate: 0.0500.. Train loss: 0.0114.. Train acc: 0.9968.. Val loss: 3.4364.. Val acc: 0.5715\n",
            "Epoch 224/300.. Learning rate: 0.0500.. Train loss: 0.0094.. Train acc: 0.9972.. Val loss: 3.4886.. Val acc: 0.5758\n",
            "Epoch 225/300.. Learning rate: 0.0500.. Train loss: 0.0095.. Train acc: 0.9970.. Val loss: 3.4404.. Val acc: 0.5748\n",
            "Epoch 226/300.. Learning rate: 0.0500.. Train loss: 0.0099.. Train acc: 0.9969.. Val loss: 3.4504.. Val acc: 0.5730\n",
            "Epoch 227/300.. Learning rate: 0.0500.. Train loss: 0.0105.. Train acc: 0.9967.. Val loss: 3.4452.. Val acc: 0.5716\n",
            "Epoch 228/300.. Learning rate: 0.0500.. Train loss: 0.0096.. Train acc: 0.9969.. Val loss: 3.4379.. Val acc: 0.5772\n",
            "Epoch 229/300.. Learning rate: 0.0500.. Train loss: 0.0102.. Train acc: 0.9966.. Val loss: 3.4933.. Val acc: 0.5712\n",
            "Epoch 230/300.. Learning rate: 0.0500.. Train loss: 0.0092.. Train acc: 0.9971.. Val loss: 3.4861.. Val acc: 0.5704\n",
            "Epoch 231/300.. Learning rate: 0.0500.. Train loss: 0.0098.. Train acc: 0.9970.. Val loss: 3.5042.. Val acc: 0.5773\n",
            "Epoch 232/300.. Learning rate: 0.0500.. Train loss: 0.0087.. Train acc: 0.9972.. Val loss: 3.4578.. Val acc: 0.5784\n",
            "Epoch 233/300.. Learning rate: 0.0500.. Train loss: 0.0108.. Train acc: 0.9965.. Val loss: 3.4736.. Val acc: 0.5778\n",
            "Epoch 234/300.. Learning rate: 0.0500.. Train loss: 0.0084.. Train acc: 0.9976.. Val loss: 3.4559.. Val acc: 0.5758\n",
            "Epoch 235/300.. Learning rate: 0.0500.. Train loss: 0.0095.. Train acc: 0.9971.. Val loss: 3.4636.. Val acc: 0.5804\n",
            "Epoch 236/300.. Learning rate: 0.0500.. Train loss: 0.0116.. Train acc: 0.9963.. Val loss: 3.5315.. Val acc: 0.5717\n",
            "Epoch 237/300.. Learning rate: 0.0500.. Train loss: 0.0094.. Train acc: 0.9968.. Val loss: 3.5090.. Val acc: 0.5689\n",
            "Epoch 238/300.. Learning rate: 0.0500.. Train loss: 0.0093.. Train acc: 0.9974.. Val loss: 3.5412.. Val acc: 0.5743\n",
            "Epoch 239/300.. Learning rate: 0.0500.. Train loss: 0.0090.. Train acc: 0.9970.. Val loss: 3.4871.. Val acc: 0.5723\n",
            "Epoch 240/300.. Learning rate: 0.0500.. Train loss: 0.0084.. Train acc: 0.9976.. Val loss: 3.5148.. Val acc: 0.5695\n",
            "Epoch 241/300.. Learning rate: 0.0500.. Train loss: 0.0075.. Train acc: 0.9977.. Val loss: 3.5282.. Val acc: 0.5724\n",
            "Epoch 242/300.. Learning rate: 0.0500.. Train loss: 0.0068.. Train acc: 0.9979.. Val loss: 3.5271.. Val acc: 0.5742\n",
            "Epoch 243/300.. Learning rate: 0.0500.. Train loss: 0.0097.. Train acc: 0.9970.. Val loss: 3.5357.. Val acc: 0.5699\n",
            "Epoch 244/300.. Learning rate: 0.0500.. Train loss: 0.0113.. Train acc: 0.9964.. Val loss: 3.5738.. Val acc: 0.5719\n",
            "Epoch 245/300.. Learning rate: 0.0500.. Train loss: 0.0102.. Train acc: 0.9966.. Val loss: 3.5634.. Val acc: 0.5675\n",
            "Epoch 246/300.. Learning rate: 0.0500.. Train loss: 0.0100.. Train acc: 0.9968.. Val loss: 3.5181.. Val acc: 0.5723\n",
            "Epoch 247/300.. Learning rate: 0.0500.. Train loss: 0.0089.. Train acc: 0.9972.. Val loss: 3.5389.. Val acc: 0.5713\n",
            "Epoch 248/300.. Learning rate: 0.0500.. Train loss: 0.0083.. Train acc: 0.9972.. Val loss: 3.5359.. Val acc: 0.5765\n",
            "Epoch 249/300.. Learning rate: 0.0500.. Train loss: 0.0094.. Train acc: 0.9969.. Val loss: 3.5234.. Val acc: 0.5714\n",
            "Epoch 250/300.. Learning rate: 0.0500.. Train loss: 0.0094.. Train acc: 0.9972.. Val loss: 3.5332.. Val acc: 0.5753\n",
            "Epoch 251/300.. Learning rate: 0.0500.. Train loss: 0.0077.. Train acc: 0.9977.. Val loss: 3.5145.. Val acc: 0.5759\n",
            "Epoch 252/300.. Learning rate: 0.0500.. Train loss: 0.0093.. Train acc: 0.9969.. Val loss: 3.5497.. Val acc: 0.5734\n",
            "Epoch 253/300.. Learning rate: 0.0500.. Train loss: 0.0089.. Train acc: 0.9973.. Val loss: 3.5521.. Val acc: 0.5712\n",
            "Epoch 254/300.. Learning rate: 0.0500.. Train loss: 0.0076.. Train acc: 0.9980.. Val loss: 3.5397.. Val acc: 0.5779\n",
            "Epoch 255/300.. Learning rate: 0.0500.. Train loss: 0.0081.. Train acc: 0.9973.. Val loss: 3.5169.. Val acc: 0.5743\n",
            "Epoch 256/300.. Learning rate: 0.0500.. Train loss: 0.0076.. Train acc: 0.9977.. Val loss: 3.5025.. Val acc: 0.5745\n",
            "Epoch 257/300.. Learning rate: 0.0500.. Train loss: 0.0083.. Train acc: 0.9970.. Val loss: 3.5021.. Val acc: 0.5732\n",
            "Epoch 258/300.. Learning rate: 0.0500.. Train loss: 0.0091.. Train acc: 0.9972.. Val loss: 3.5068.. Val acc: 0.5747\n",
            "Epoch 259/300.. Learning rate: 0.0500.. Train loss: 0.0081.. Train acc: 0.9975.. Val loss: 3.5113.. Val acc: 0.5761\n",
            "Epoch 260/300.. Learning rate: 0.0500.. Train loss: 0.0090.. Train acc: 0.9971.. Val loss: 3.5303.. Val acc: 0.5706\n",
            "Epoch 261/300.. Learning rate: 0.0500.. Train loss: 0.0103.. Train acc: 0.9970.. Val loss: 3.5508.. Val acc: 0.5673\n",
            "Epoch 262/300.. Learning rate: 0.0500.. Train loss: 0.0094.. Train acc: 0.9971.. Val loss: 3.5345.. Val acc: 0.5694\n",
            "Epoch 263/300.. Learning rate: 0.0500.. Train loss: 0.0082.. Train acc: 0.9973.. Val loss: 3.5165.. Val acc: 0.5719\n",
            "Epoch 264/300.. Learning rate: 0.0500.. Train loss: 0.0090.. Train acc: 0.9973.. Val loss: 3.5444.. Val acc: 0.5712\n",
            "Epoch 265/300.. Learning rate: 0.0500.. Train loss: 0.0071.. Train acc: 0.9978.. Val loss: 3.5505.. Val acc: 0.5728\n",
            "Epoch 266/300.. Learning rate: 0.0500.. Train loss: 0.0077.. Train acc: 0.9976.. Val loss: 3.5934.. Val acc: 0.5746\n",
            "Epoch 267/300.. Learning rate: 0.0500.. Train loss: 0.0086.. Train acc: 0.9972.. Val loss: 3.5530.. Val acc: 0.5798\n",
            "Epoch 268/300.. Learning rate: 0.0500.. Train loss: 0.0080.. Train acc: 0.9976.. Val loss: 3.5374.. Val acc: 0.5756\n",
            "Epoch 269/300.. Learning rate: 0.0500.. Train loss: 0.0081.. Train acc: 0.9975.. Val loss: 3.5890.. Val acc: 0.5718\n",
            "Epoch 270/300.. Learning rate: 0.0500.. Train loss: 0.0074.. Train acc: 0.9977.. Val loss: 3.5491.. Val acc: 0.5728\n",
            "Epoch 271/300.. Learning rate: 0.0500.. Train loss: 0.0064.. Train acc: 0.9982.. Val loss: 3.5426.. Val acc: 0.5756\n",
            "Epoch 272/300.. Learning rate: 0.0500.. Train loss: 0.0074.. Train acc: 0.9974.. Val loss: 3.5599.. Val acc: 0.5748\n",
            "Epoch 273/300.. Learning rate: 0.0500.. Train loss: 0.0080.. Train acc: 0.9976.. Val loss: 3.5961.. Val acc: 0.5697\n",
            "Epoch 274/300.. Learning rate: 0.0500.. Train loss: 0.0088.. Train acc: 0.9971.. Val loss: 3.5553.. Val acc: 0.5751\n",
            "Epoch 275/300.. Learning rate: 0.0500.. Train loss: 0.0080.. Train acc: 0.9973.. Val loss: 3.5666.. Val acc: 0.5785\n",
            "Epoch 276/300.. Learning rate: 0.0500.. Train loss: 0.0073.. Train acc: 0.9975.. Val loss: 3.5861.. Val acc: 0.5722\n",
            "Epoch 277/300.. Learning rate: 0.0500.. Train loss: 0.0079.. Train acc: 0.9977.. Val loss: 3.6024.. Val acc: 0.5726\n",
            "Epoch 278/300.. Learning rate: 0.0500.. Train loss: 0.0098.. Train acc: 0.9969.. Val loss: 3.6038.. Val acc: 0.5721\n",
            "Epoch 279/300.. Learning rate: 0.0500.. Train loss: 0.0085.. Train acc: 0.9973.. Val loss: 3.6076.. Val acc: 0.5727\n",
            "Epoch 280/300.. Learning rate: 0.0500.. Train loss: 0.0088.. Train acc: 0.9971.. Val loss: 3.5651.. Val acc: 0.5773\n",
            "Epoch 281/300.. Learning rate: 0.0500.. Train loss: 0.0074.. Train acc: 0.9974.. Val loss: 3.5703.. Val acc: 0.5749\n",
            "Epoch 282/300.. Learning rate: 0.0500.. Train loss: 0.0084.. Train acc: 0.9973.. Val loss: 3.5505.. Val acc: 0.5757\n",
            "Epoch 283/300.. Learning rate: 0.0500.. Train loss: 0.0075.. Train acc: 0.9977.. Val loss: 3.5815.. Val acc: 0.5782\n",
            "Epoch 284/300.. Learning rate: 0.0500.. Train loss: 0.0069.. Train acc: 0.9979.. Val loss: 3.5538.. Val acc: 0.5773\n",
            "Epoch 285/300.. Learning rate: 0.0500.. Train loss: 0.0068.. Train acc: 0.9978.. Val loss: 3.5710.. Val acc: 0.5736\n",
            "Epoch 286/300.. Learning rate: 0.0500.. Train loss: 0.0075.. Train acc: 0.9974.. Val loss: 3.5729.. Val acc: 0.5772\n",
            "Epoch 287/300.. Learning rate: 0.0500.. Train loss: 0.0091.. Train acc: 0.9970.. Val loss: 3.5845.. Val acc: 0.5755\n",
            "Epoch 288/300.. Learning rate: 0.0500.. Train loss: 0.0068.. Train acc: 0.9978.. Val loss: 3.5985.. Val acc: 0.5754\n",
            "Epoch 289/300.. Learning rate: 0.0500.. Train loss: 0.0081.. Train acc: 0.9975.. Val loss: 3.5877.. Val acc: 0.5746\n",
            "Epoch 290/300.. Learning rate: 0.0500.. Train loss: 0.0077.. Train acc: 0.9976.. Val loss: 3.6147.. Val acc: 0.5713\n",
            "Epoch 291/300.. Learning rate: 0.0500.. Train loss: 0.0077.. Train acc: 0.9978.. Val loss: 3.5829.. Val acc: 0.5776\n",
            "Epoch 292/300.. Learning rate: 0.0500.. Train loss: 0.0069.. Train acc: 0.9975.. Val loss: 3.5735.. Val acc: 0.5776\n",
            "Epoch 293/300.. Learning rate: 0.0500.. Train loss: 0.0091.. Train acc: 0.9970.. Val loss: 3.6159.. Val acc: 0.5735\n",
            "Epoch 294/300.. Learning rate: 0.0500.. Train loss: 0.0077.. Train acc: 0.9973.. Val loss: 3.6045.. Val acc: 0.5755\n",
            "Epoch 295/300.. Learning rate: 0.0500.. Train loss: 0.0074.. Train acc: 0.9981.. Val loss: 3.5873.. Val acc: 0.5737\n",
            "Epoch 296/300.. Learning rate: 0.0500.. Train loss: 0.0068.. Train acc: 0.9979.. Val loss: 3.6000.. Val acc: 0.5751\n",
            "Epoch 297/300.. Learning rate: 0.0500.. Train loss: 0.0055.. Train acc: 0.9982.. Val loss: 3.6022.. Val acc: 0.5759\n",
            "Epoch 298/300.. Learning rate: 0.0500.. Train loss: 0.0064.. Train acc: 0.9977.. Val loss: 3.5962.. Val acc: 0.5773\n",
            "Epoch 299/300.. Learning rate: 0.0500.. Train loss: 0.0059.. Train acc: 0.9979.. Val loss: 3.6137.. Val acc: 0.5734\n",
            "Epoch 300/300.. Learning rate: 0.0500.. Train loss: 0.0069.. Train acc: 0.9980.. Val loss: 3.5976.. Val acc: 0.5727\n",
            "Image to be saved: lr=0.05_epochs=3001.png\n",
            "Test loss:  3.648585498046875\n",
            "Test acc:  0.567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate schedule(Cosine *Annealing*)"
      ],
      "metadata": {
        "id": "-JxI9dJAQPIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 300 \\\n",
        "--lr 0.05 --wd 0.0\\\n",
        "--lr_scheduler \\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.05_epochs=300_lr_sch_cos.png \\\n",
        "--test"
      ],
      "metadata": {
        "id": "DsBikD6HPu0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b0e4f2-4ae4-4b62-be28-9ccc0d9238f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to mydata2/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:12<00:00, 13157345.86it/s]\n",
            "Extracting mydata2/cifar-100-python.tar.gz to mydata2\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=300, lr=0.05, wd=0.0, fig_name='lr=0.05_epochs=300_lr_sch_cos.png', lr_scheduler=True, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIAGRAM FOLDER CREATED /content\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1462.. Train acc: 0.0627.. Val loss: 3.8218.. Val acc: 0.1079\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6778.. Train acc: 0.1238.. Val loss: 3.4408.. Val acc: 0.1629\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.4098.. Train acc: 0.1743.. Val loss: 3.3103.. Val acc: 0.1853\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.1863.. Train acc: 0.2128.. Val loss: 3.1060.. Val acc: 0.2308\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.9926.. Train acc: 0.2499.. Val loss: 2.9517.. Val acc: 0.2674\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.8156.. Train acc: 0.2812.. Val loss: 2.7502.. Val acc: 0.3033\n",
            "Epoch 7/300.. Learning rate: 0.0499.. Train loss: 2.6448.. Train acc: 0.3167.. Val loss: 2.6032.. Val acc: 0.3237\n",
            "Epoch 8/300.. Learning rate: 0.0499.. Train loss: 2.4968.. Train acc: 0.3457.. Val loss: 2.5079.. Val acc: 0.3476\n",
            "Epoch 9/300.. Learning rate: 0.0499.. Train loss: 2.3561.. Train acc: 0.3770.. Val loss: 2.4778.. Val acc: 0.3559\n",
            "Epoch 10/300.. Learning rate: 0.0499.. Train loss: 2.2351.. Train acc: 0.4013.. Val loss: 2.3476.. Val acc: 0.3861\n",
            "Epoch 11/300.. Learning rate: 0.0498.. Train loss: 2.1230.. Train acc: 0.4246.. Val loss: 2.2934.. Val acc: 0.4030\n",
            "Epoch 12/300.. Learning rate: 0.0498.. Train loss: 2.0303.. Train acc: 0.4463.. Val loss: 2.2580.. Val acc: 0.4088\n",
            "Epoch 13/300.. Learning rate: 0.0498.. Train loss: 1.9332.. Train acc: 0.4681.. Val loss: 2.2326.. Val acc: 0.4160\n",
            "Epoch 14/300.. Learning rate: 0.0497.. Train loss: 1.8471.. Train acc: 0.4903.. Val loss: 2.0346.. Val acc: 0.4532\n",
            "Epoch 15/300.. Learning rate: 0.0497.. Train loss: 1.7818.. Train acc: 0.5029.. Val loss: 2.0439.. Val acc: 0.4606\n",
            "Epoch 16/300.. Learning rate: 0.0497.. Train loss: 1.7000.. Train acc: 0.5221.. Val loss: 2.0855.. Val acc: 0.4583\n",
            "Epoch 17/300.. Learning rate: 0.0496.. Train loss: 1.6209.. Train acc: 0.5396.. Val loss: 1.9570.. Val acc: 0.4799\n",
            "Epoch 18/300.. Learning rate: 0.0496.. Train loss: 1.5607.. Train acc: 0.5556.. Val loss: 1.9535.. Val acc: 0.4790\n",
            "Epoch 19/300.. Learning rate: 0.0495.. Train loss: 1.4975.. Train acc: 0.5682.. Val loss: 1.9638.. Val acc: 0.4838\n",
            "Epoch 20/300.. Learning rate: 0.0495.. Train loss: 1.4296.. Train acc: 0.5842.. Val loss: 2.0147.. Val acc: 0.4819\n",
            "Epoch 21/300.. Learning rate: 0.0494.. Train loss: 1.3768.. Train acc: 0.5994.. Val loss: 1.9645.. Val acc: 0.4909\n",
            "Epoch 22/300.. Learning rate: 0.0493.. Train loss: 1.3267.. Train acc: 0.6073.. Val loss: 1.9316.. Val acc: 0.4960\n",
            "Epoch 23/300.. Learning rate: 0.0493.. Train loss: 1.2725.. Train acc: 0.6248.. Val loss: 1.9045.. Val acc: 0.5104\n",
            "Epoch 24/300.. Learning rate: 0.0492.. Train loss: 1.2185.. Train acc: 0.6372.. Val loss: 1.9471.. Val acc: 0.5107\n",
            "Epoch 25/300.. Learning rate: 0.0492.. Train loss: 1.1622.. Train acc: 0.6532.. Val loss: 1.9781.. Val acc: 0.5087\n",
            "Epoch 26/300.. Learning rate: 0.0491.. Train loss: 1.1247.. Train acc: 0.6614.. Val loss: 1.9865.. Val acc: 0.5067\n",
            "Epoch 27/300.. Learning rate: 0.0490.. Train loss: 1.0686.. Train acc: 0.6743.. Val loss: 1.9926.. Val acc: 0.5126\n",
            "Epoch 28/300.. Learning rate: 0.0489.. Train loss: 1.0073.. Train acc: 0.6904.. Val loss: 1.9951.. Val acc: 0.5108\n",
            "Epoch 29/300.. Learning rate: 0.0489.. Train loss: 0.9632.. Train acc: 0.7038.. Val loss: 1.9922.. Val acc: 0.5198\n",
            "Epoch 30/300.. Learning rate: 0.0488.. Train loss: 0.9137.. Train acc: 0.7155.. Val loss: 2.1216.. Val acc: 0.5025\n",
            "Epoch 31/300.. Learning rate: 0.0487.. Train loss: 0.8600.. Train acc: 0.7300.. Val loss: 2.0706.. Val acc: 0.5196\n",
            "Epoch 32/300.. Learning rate: 0.0486.. Train loss: 0.8224.. Train acc: 0.7398.. Val loss: 2.0791.. Val acc: 0.5254\n",
            "Epoch 33/300.. Learning rate: 0.0485.. Train loss: 0.7674.. Train acc: 0.7588.. Val loss: 2.1085.. Val acc: 0.5233\n",
            "Epoch 34/300.. Learning rate: 0.0484.. Train loss: 0.7159.. Train acc: 0.7740.. Val loss: 2.1318.. Val acc: 0.5232\n",
            "Epoch 35/300.. Learning rate: 0.0483.. Train loss: 0.6828.. Train acc: 0.7804.. Val loss: 2.1823.. Val acc: 0.5191\n",
            "Epoch 36/300.. Learning rate: 0.0482.. Train loss: 0.6311.. Train acc: 0.7978.. Val loss: 2.2226.. Val acc: 0.5254\n",
            "Epoch 37/300.. Learning rate: 0.0481.. Train loss: 0.5989.. Train acc: 0.8063.. Val loss: 2.2661.. Val acc: 0.5140\n",
            "Epoch 38/300.. Learning rate: 0.0480.. Train loss: 0.5604.. Train acc: 0.8185.. Val loss: 2.2770.. Val acc: 0.5242\n",
            "Epoch 39/300.. Learning rate: 0.0479.. Train loss: 0.5178.. Train acc: 0.8317.. Val loss: 2.2650.. Val acc: 0.5254\n",
            "Epoch 40/300.. Learning rate: 0.0478.. Train loss: 0.4881.. Train acc: 0.8417.. Val loss: 2.3931.. Val acc: 0.5161\n",
            "Epoch 41/300.. Learning rate: 0.0477.. Train loss: 0.4526.. Train acc: 0.8537.. Val loss: 2.3494.. Val acc: 0.5261\n",
            "Epoch 42/300.. Learning rate: 0.0476.. Train loss: 0.4374.. Train acc: 0.8579.. Val loss: 2.3607.. Val acc: 0.5301\n",
            "Epoch 43/300.. Learning rate: 0.0475.. Train loss: 0.3986.. Train acc: 0.8703.. Val loss: 2.4438.. Val acc: 0.5273\n",
            "Epoch 44/300.. Learning rate: 0.0474.. Train loss: 0.3734.. Train acc: 0.8781.. Val loss: 2.3712.. Val acc: 0.5286\n",
            "Epoch 45/300.. Learning rate: 0.0473.. Train loss: 0.3534.. Train acc: 0.8839.. Val loss: 2.4777.. Val acc: 0.5283\n",
            "Epoch 46/300.. Learning rate: 0.0472.. Train loss: 0.3340.. Train acc: 0.8884.. Val loss: 2.4649.. Val acc: 0.5333\n",
            "Epoch 47/300.. Learning rate: 0.0470.. Train loss: 0.3127.. Train acc: 0.8985.. Val loss: 2.5522.. Val acc: 0.5323\n",
            "Epoch 48/300.. Learning rate: 0.0469.. Train loss: 0.2924.. Train acc: 0.9044.. Val loss: 2.4973.. Val acc: 0.5318\n",
            "Epoch 49/300.. Learning rate: 0.0468.. Train loss: 0.2705.. Train acc: 0.9109.. Val loss: 2.5397.. Val acc: 0.5295\n",
            "Epoch 50/300.. Learning rate: 0.0467.. Train loss: 0.2527.. Train acc: 0.9179.. Val loss: 2.6061.. Val acc: 0.5319\n",
            "Epoch 51/300.. Learning rate: 0.0465.. Train loss: 0.2329.. Train acc: 0.9232.. Val loss: 2.6257.. Val acc: 0.5396\n",
            "Epoch 52/300.. Learning rate: 0.0464.. Train loss: 0.2182.. Train acc: 0.9291.. Val loss: 2.6029.. Val acc: 0.5347\n",
            "Epoch 53/300.. Learning rate: 0.0463.. Train loss: 0.2159.. Train acc: 0.9311.. Val loss: 2.6614.. Val acc: 0.5394\n",
            "Epoch 54/300.. Learning rate: 0.0461.. Train loss: 0.2106.. Train acc: 0.9325.. Val loss: 2.6696.. Val acc: 0.5352\n",
            "Epoch 55/300.. Learning rate: 0.0460.. Train loss: 0.1997.. Train acc: 0.9361.. Val loss: 2.7064.. Val acc: 0.5355\n",
            "Epoch 56/300.. Learning rate: 0.0458.. Train loss: 0.1820.. Train acc: 0.9412.. Val loss: 2.7087.. Val acc: 0.5342\n",
            "Epoch 57/300.. Learning rate: 0.0457.. Train loss: 0.1702.. Train acc: 0.9453.. Val loss: 2.6684.. Val acc: 0.5414\n",
            "Epoch 58/300.. Learning rate: 0.0455.. Train loss: 0.1567.. Train acc: 0.9496.. Val loss: 2.6979.. Val acc: 0.5445\n",
            "Epoch 59/300.. Learning rate: 0.0454.. Train loss: 0.1430.. Train acc: 0.9537.. Val loss: 2.7538.. Val acc: 0.5434\n",
            "Epoch 60/300.. Learning rate: 0.0452.. Train loss: 0.1447.. Train acc: 0.9531.. Val loss: 2.7806.. Val acc: 0.5385\n",
            "Epoch 61/300.. Learning rate: 0.0451.. Train loss: 0.1305.. Train acc: 0.9588.. Val loss: 2.7663.. Val acc: 0.5401\n",
            "Epoch 62/300.. Learning rate: 0.0449.. Train loss: 0.1291.. Train acc: 0.9583.. Val loss: 2.8121.. Val acc: 0.5378\n",
            "Epoch 63/300.. Learning rate: 0.0448.. Train loss: 0.1163.. Train acc: 0.9630.. Val loss: 2.8803.. Val acc: 0.5326\n",
            "Epoch 64/300.. Learning rate: 0.0446.. Train loss: 0.1062.. Train acc: 0.9659.. Val loss: 2.8219.. Val acc: 0.5445\n",
            "Epoch 65/300.. Learning rate: 0.0444.. Train loss: 0.1075.. Train acc: 0.9657.. Val loss: 2.7765.. Val acc: 0.5475\n",
            "Epoch 66/300.. Learning rate: 0.0443.. Train loss: 0.0963.. Train acc: 0.9694.. Val loss: 2.8647.. Val acc: 0.5414\n",
            "Epoch 67/300.. Learning rate: 0.0441.. Train loss: 0.0972.. Train acc: 0.9692.. Val loss: 2.8937.. Val acc: 0.5409\n",
            "Epoch 68/300.. Learning rate: 0.0439.. Train loss: 0.0929.. Train acc: 0.9715.. Val loss: 2.8651.. Val acc: 0.5455\n",
            "Epoch 69/300.. Learning rate: 0.0438.. Train loss: 0.0883.. Train acc: 0.9719.. Val loss: 2.8680.. Val acc: 0.5447\n",
            "Epoch 70/300.. Learning rate: 0.0436.. Train loss: 0.0765.. Train acc: 0.9755.. Val loss: 2.9037.. Val acc: 0.5445\n",
            "Epoch 71/300.. Learning rate: 0.0434.. Train loss: 0.0755.. Train acc: 0.9764.. Val loss: 2.8906.. Val acc: 0.5488\n",
            "Epoch 72/300.. Learning rate: 0.0432.. Train loss: 0.0694.. Train acc: 0.9784.. Val loss: 2.8850.. Val acc: 0.5499\n",
            "Epoch 73/300.. Learning rate: 0.0430.. Train loss: 0.0710.. Train acc: 0.9775.. Val loss: 2.9920.. Val acc: 0.5479\n",
            "Epoch 74/300.. Learning rate: 0.0429.. Train loss: 0.0738.. Train acc: 0.9771.. Val loss: 2.9123.. Val acc: 0.5495\n",
            "Epoch 75/300.. Learning rate: 0.0427.. Train loss: 0.0727.. Train acc: 0.9764.. Val loss: 2.9409.. Val acc: 0.5465\n",
            "Epoch 76/300.. Learning rate: 0.0425.. Train loss: 0.0637.. Train acc: 0.9802.. Val loss: 2.9527.. Val acc: 0.5493\n",
            "Epoch 77/300.. Learning rate: 0.0423.. Train loss: 0.0652.. Train acc: 0.9795.. Val loss: 2.9502.. Val acc: 0.5480\n",
            "Epoch 78/300.. Learning rate: 0.0421.. Train loss: 0.0659.. Train acc: 0.9791.. Val loss: 3.0143.. Val acc: 0.5470\n",
            "Epoch 79/300.. Learning rate: 0.0419.. Train loss: 0.0575.. Train acc: 0.9825.. Val loss: 3.0098.. Val acc: 0.5466\n",
            "Epoch 80/300.. Learning rate: 0.0417.. Train loss: 0.0553.. Train acc: 0.9826.. Val loss: 2.9946.. Val acc: 0.5516\n",
            "Epoch 81/300.. Learning rate: 0.0415.. Train loss: 0.0507.. Train acc: 0.9843.. Val loss: 2.9677.. Val acc: 0.5538\n",
            "Epoch 82/300.. Learning rate: 0.0413.. Train loss: 0.0561.. Train acc: 0.9817.. Val loss: 3.0273.. Val acc: 0.5500\n",
            "Epoch 83/300.. Learning rate: 0.0411.. Train loss: 0.0457.. Train acc: 0.9858.. Val loss: 3.0406.. Val acc: 0.5523\n",
            "Epoch 84/300.. Learning rate: 0.0409.. Train loss: 0.0473.. Train acc: 0.9853.. Val loss: 3.0291.. Val acc: 0.5531\n",
            "Epoch 85/300.. Learning rate: 0.0407.. Train loss: 0.0434.. Train acc: 0.9869.. Val loss: 3.0649.. Val acc: 0.5560\n",
            "Epoch 86/300.. Learning rate: 0.0405.. Train loss: 0.0414.. Train acc: 0.9875.. Val loss: 3.0731.. Val acc: 0.5536\n",
            "Epoch 87/300.. Learning rate: 0.0403.. Train loss: 0.0460.. Train acc: 0.9856.. Val loss: 3.0617.. Val acc: 0.5511\n",
            "Epoch 88/300.. Learning rate: 0.0401.. Train loss: 0.0393.. Train acc: 0.9880.. Val loss: 3.0663.. Val acc: 0.5588\n",
            "Epoch 89/300.. Learning rate: 0.0399.. Train loss: 0.0390.. Train acc: 0.9879.. Val loss: 3.0662.. Val acc: 0.5577\n",
            "Epoch 90/300.. Learning rate: 0.0397.. Train loss: 0.0332.. Train acc: 0.9899.. Val loss: 3.0606.. Val acc: 0.5578\n",
            "Epoch 91/300.. Learning rate: 0.0395.. Train loss: 0.0336.. Train acc: 0.9902.. Val loss: 3.1011.. Val acc: 0.5568\n",
            "Epoch 92/300.. Learning rate: 0.0393.. Train loss: 0.0290.. Train acc: 0.9922.. Val loss: 3.0824.. Val acc: 0.5546\n",
            "Epoch 93/300.. Learning rate: 0.0391.. Train loss: 0.0316.. Train acc: 0.9902.. Val loss: 3.1510.. Val acc: 0.5538\n",
            "Epoch 94/300.. Learning rate: 0.0388.. Train loss: 0.0270.. Train acc: 0.9921.. Val loss: 3.1042.. Val acc: 0.5624\n",
            "Epoch 95/300.. Learning rate: 0.0386.. Train loss: 0.0297.. Train acc: 0.9911.. Val loss: 3.1192.. Val acc: 0.5583\n",
            "Epoch 96/300.. Learning rate: 0.0384.. Train loss: 0.0274.. Train acc: 0.9918.. Val loss: 3.1111.. Val acc: 0.5585\n",
            "Epoch 97/300.. Learning rate: 0.0382.. Train loss: 0.0277.. Train acc: 0.9920.. Val loss: 3.0780.. Val acc: 0.5587\n",
            "Epoch 98/300.. Learning rate: 0.0380.. Train loss: 0.0250.. Train acc: 0.9931.. Val loss: 3.1111.. Val acc: 0.5563\n",
            "Epoch 99/300.. Learning rate: 0.0377.. Train loss: 0.0267.. Train acc: 0.9921.. Val loss: 3.1095.. Val acc: 0.5612\n",
            "Epoch 100/300.. Learning rate: 0.0375.. Train loss: 0.0241.. Train acc: 0.9932.. Val loss: 3.1405.. Val acc: 0.5603\n",
            "Epoch 101/300.. Learning rate: 0.0373.. Train loss: 0.0226.. Train acc: 0.9935.. Val loss: 3.1379.. Val acc: 0.5546\n",
            "Epoch 102/300.. Learning rate: 0.0370.. Train loss: 0.0225.. Train acc: 0.9931.. Val loss: 3.1315.. Val acc: 0.5622\n",
            "Epoch 103/300.. Learning rate: 0.0368.. Train loss: 0.0225.. Train acc: 0.9935.. Val loss: 3.1509.. Val acc: 0.5588\n",
            "Epoch 104/300.. Learning rate: 0.0366.. Train loss: 0.0199.. Train acc: 0.9942.. Val loss: 3.1695.. Val acc: 0.5568\n",
            "Epoch 105/300.. Learning rate: 0.0364.. Train loss: 0.0201.. Train acc: 0.9940.. Val loss: 3.1631.. Val acc: 0.5596\n",
            "Epoch 106/300.. Learning rate: 0.0361.. Train loss: 0.0177.. Train acc: 0.9947.. Val loss: 3.1777.. Val acc: 0.5548\n",
            "Epoch 107/300.. Learning rate: 0.0359.. Train loss: 0.0176.. Train acc: 0.9950.. Val loss: 3.1940.. Val acc: 0.5588\n",
            "Epoch 108/300.. Learning rate: 0.0356.. Train loss: 0.0159.. Train acc: 0.9955.. Val loss: 3.2014.. Val acc: 0.5638\n",
            "Epoch 109/300.. Learning rate: 0.0354.. Train loss: 0.0180.. Train acc: 0.9950.. Val loss: 3.1967.. Val acc: 0.5585\n",
            "Epoch 110/300.. Learning rate: 0.0352.. Train loss: 0.0168.. Train acc: 0.9954.. Val loss: 3.1852.. Val acc: 0.5615\n",
            "Epoch 111/300.. Learning rate: 0.0349.. Train loss: 0.0149.. Train acc: 0.9957.. Val loss: 3.1675.. Val acc: 0.5623\n",
            "Epoch 112/300.. Learning rate: 0.0347.. Train loss: 0.0141.. Train acc: 0.9959.. Val loss: 3.2215.. Val acc: 0.5651\n",
            "Epoch 113/300.. Learning rate: 0.0344.. Train loss: 0.0124.. Train acc: 0.9966.. Val loss: 3.1866.. Val acc: 0.5629\n",
            "Epoch 114/300.. Learning rate: 0.0342.. Train loss: 0.0120.. Train acc: 0.9964.. Val loss: 3.1901.. Val acc: 0.5579\n",
            "Epoch 115/300.. Learning rate: 0.0340.. Train loss: 0.0132.. Train acc: 0.9963.. Val loss: 3.2363.. Val acc: 0.5581\n",
            "Epoch 116/300.. Learning rate: 0.0337.. Train loss: 0.0125.. Train acc: 0.9966.. Val loss: 3.2119.. Val acc: 0.5567\n",
            "Epoch 117/300.. Learning rate: 0.0335.. Train loss: 0.0120.. Train acc: 0.9965.. Val loss: 3.2418.. Val acc: 0.5599\n",
            "Epoch 118/300.. Learning rate: 0.0332.. Train loss: 0.0116.. Train acc: 0.9969.. Val loss: 3.2340.. Val acc: 0.5572\n",
            "Epoch 119/300.. Learning rate: 0.0330.. Train loss: 0.0111.. Train acc: 0.9972.. Val loss: 3.2062.. Val acc: 0.5643\n",
            "Epoch 120/300.. Learning rate: 0.0327.. Train loss: 0.0098.. Train acc: 0.9974.. Val loss: 3.2061.. Val acc: 0.5630\n",
            "Epoch 121/300.. Learning rate: 0.0325.. Train loss: 0.0113.. Train acc: 0.9971.. Val loss: 3.2391.. Val acc: 0.5599\n",
            "Epoch 122/300.. Learning rate: 0.0322.. Train loss: 0.0101.. Train acc: 0.9972.. Val loss: 3.2051.. Val acc: 0.5652\n",
            "Epoch 123/300.. Learning rate: 0.0320.. Train loss: 0.0094.. Train acc: 0.9977.. Val loss: 3.2091.. Val acc: 0.5686\n",
            "Epoch 124/300.. Learning rate: 0.0317.. Train loss: 0.0084.. Train acc: 0.9980.. Val loss: 3.2431.. Val acc: 0.5637\n",
            "Epoch 125/300.. Learning rate: 0.0315.. Train loss: 0.0086.. Train acc: 0.9975.. Val loss: 3.2056.. Val acc: 0.5667\n",
            "Epoch 126/300.. Learning rate: 0.0312.. Train loss: 0.0081.. Train acc: 0.9979.. Val loss: 3.2271.. Val acc: 0.5662\n",
            "Epoch 127/300.. Learning rate: 0.0310.. Train loss: 0.0090.. Train acc: 0.9975.. Val loss: 3.2522.. Val acc: 0.5638\n",
            "Epoch 128/300.. Learning rate: 0.0307.. Train loss: 0.0084.. Train acc: 0.9980.. Val loss: 3.2325.. Val acc: 0.5669\n",
            "Epoch 129/300.. Learning rate: 0.0305.. Train loss: 0.0080.. Train acc: 0.9979.. Val loss: 3.2235.. Val acc: 0.5678\n",
            "Epoch 130/300.. Learning rate: 0.0302.. Train loss: 0.0076.. Train acc: 0.9978.. Val loss: 3.2580.. Val acc: 0.5654\n",
            "Epoch 131/300.. Learning rate: 0.0299.. Train loss: 0.0073.. Train acc: 0.9981.. Val loss: 3.2180.. Val acc: 0.5665\n",
            "Epoch 132/300.. Learning rate: 0.0297.. Train loss: 0.0072.. Train acc: 0.9982.. Val loss: 3.2426.. Val acc: 0.5678\n",
            "Epoch 133/300.. Learning rate: 0.0294.. Train loss: 0.0068.. Train acc: 0.9982.. Val loss: 3.2588.. Val acc: 0.5670\n",
            "Epoch 134/300.. Learning rate: 0.0292.. Train loss: 0.0060.. Train acc: 0.9984.. Val loss: 3.2365.. Val acc: 0.5640\n",
            "Epoch 135/300.. Learning rate: 0.0289.. Train loss: 0.0066.. Train acc: 0.9983.. Val loss: 3.2436.. Val acc: 0.5654\n",
            "Epoch 136/300.. Learning rate: 0.0287.. Train loss: 0.0065.. Train acc: 0.9983.. Val loss: 3.2604.. Val acc: 0.5662\n",
            "Epoch 137/300.. Learning rate: 0.0284.. Train loss: 0.0057.. Train acc: 0.9987.. Val loss: 3.2592.. Val acc: 0.5671\n",
            "Epoch 138/300.. Learning rate: 0.0281.. Train loss: 0.0052.. Train acc: 0.9989.. Val loss: 3.2600.. Val acc: 0.5633\n",
            "Epoch 139/300.. Learning rate: 0.0279.. Train loss: 0.0055.. Train acc: 0.9983.. Val loss: 3.2608.. Val acc: 0.5664\n",
            "Epoch 140/300.. Learning rate: 0.0276.. Train loss: 0.0056.. Train acc: 0.9984.. Val loss: 3.2597.. Val acc: 0.5652\n",
            "Epoch 141/300.. Learning rate: 0.0274.. Train loss: 0.0057.. Train acc: 0.9984.. Val loss: 3.2768.. Val acc: 0.5689\n",
            "Epoch 142/300.. Learning rate: 0.0271.. Train loss: 0.0050.. Train acc: 0.9987.. Val loss: 3.2452.. Val acc: 0.5689\n",
            "Epoch 143/300.. Learning rate: 0.0268.. Train loss: 0.0048.. Train acc: 0.9988.. Val loss: 3.2768.. Val acc: 0.5690\n",
            "Epoch 144/300.. Learning rate: 0.0266.. Train loss: 0.0046.. Train acc: 0.9989.. Val loss: 3.2635.. Val acc: 0.5679\n",
            "Epoch 145/300.. Learning rate: 0.0263.. Train loss: 0.0046.. Train acc: 0.9989.. Val loss: 3.2877.. Val acc: 0.5704\n",
            "Epoch 146/300.. Learning rate: 0.0260.. Train loss: 0.0056.. Train acc: 0.9985.. Val loss: 3.2664.. Val acc: 0.5688\n",
            "Epoch 147/300.. Learning rate: 0.0258.. Train loss: 0.0047.. Train acc: 0.9988.. Val loss: 3.2717.. Val acc: 0.5679\n",
            "Epoch 148/300.. Learning rate: 0.0255.. Train loss: 0.0053.. Train acc: 0.9986.. Val loss: 3.2816.. Val acc: 0.5679\n",
            "Epoch 149/300.. Learning rate: 0.0253.. Train loss: 0.0049.. Train acc: 0.9988.. Val loss: 3.2759.. Val acc: 0.5693\n",
            "Epoch 150/300.. Learning rate: 0.0250.. Train loss: 0.0040.. Train acc: 0.9990.. Val loss: 3.2873.. Val acc: 0.5715\n",
            "Epoch 151/300.. Learning rate: 0.0247.. Train loss: 0.0045.. Train acc: 0.9988.. Val loss: 3.2786.. Val acc: 0.5694\n",
            "Epoch 152/300.. Learning rate: 0.0245.. Train loss: 0.0040.. Train acc: 0.9990.. Val loss: 3.2827.. Val acc: 0.5658\n",
            "Epoch 153/300.. Learning rate: 0.0242.. Train loss: 0.0039.. Train acc: 0.9990.. Val loss: 3.2817.. Val acc: 0.5672\n",
            "Epoch 154/300.. Learning rate: 0.0240.. Train loss: 0.0042.. Train acc: 0.9988.. Val loss: 3.2844.. Val acc: 0.5694\n",
            "Epoch 155/300.. Learning rate: 0.0237.. Train loss: 0.0039.. Train acc: 0.9991.. Val loss: 3.2796.. Val acc: 0.5699\n",
            "Epoch 156/300.. Learning rate: 0.0234.. Train loss: 0.0037.. Train acc: 0.9989.. Val loss: 3.2864.. Val acc: 0.5695\n",
            "Epoch 157/300.. Learning rate: 0.0232.. Train loss: 0.0038.. Train acc: 0.9990.. Val loss: 3.2760.. Val acc: 0.5708\n",
            "Epoch 158/300.. Learning rate: 0.0229.. Train loss: 0.0040.. Train acc: 0.9990.. Val loss: 3.2875.. Val acc: 0.5680\n",
            "Epoch 159/300.. Learning rate: 0.0227.. Train loss: 0.0039.. Train acc: 0.9990.. Val loss: 3.2844.. Val acc: 0.5669\n",
            "Epoch 160/300.. Learning rate: 0.0224.. Train loss: 0.0034.. Train acc: 0.9992.. Val loss: 3.2844.. Val acc: 0.5702\n",
            "Epoch 161/300.. Learning rate: 0.0221.. Train loss: 0.0035.. Train acc: 0.9991.. Val loss: 3.2826.. Val acc: 0.5699\n",
            "Epoch 162/300.. Learning rate: 0.0219.. Train loss: 0.0031.. Train acc: 0.9992.. Val loss: 3.3181.. Val acc: 0.5681\n",
            "Epoch 163/300.. Learning rate: 0.0216.. Train loss: 0.0034.. Train acc: 0.9990.. Val loss: 3.2809.. Val acc: 0.5675\n",
            "Epoch 164/300.. Learning rate: 0.0214.. Train loss: 0.0032.. Train acc: 0.9991.. Val loss: 3.2777.. Val acc: 0.5691\n",
            "Epoch 165/300.. Learning rate: 0.0211.. Train loss: 0.0032.. Train acc: 0.9993.. Val loss: 3.2824.. Val acc: 0.5696\n",
            "Epoch 166/300.. Learning rate: 0.0208.. Train loss: 0.0032.. Train acc: 0.9992.. Val loss: 3.2845.. Val acc: 0.5680\n",
            "Epoch 167/300.. Learning rate: 0.0206.. Train loss: 0.0030.. Train acc: 0.9993.. Val loss: 3.2853.. Val acc: 0.5712\n",
            "Epoch 168/300.. Learning rate: 0.0203.. Train loss: 0.0033.. Train acc: 0.9991.. Val loss: 3.2997.. Val acc: 0.5720\n",
            "Epoch 169/300.. Learning rate: 0.0201.. Train loss: 0.0031.. Train acc: 0.9991.. Val loss: 3.2726.. Val acc: 0.5731\n",
            "Epoch 170/300.. Learning rate: 0.0198.. Train loss: 0.0026.. Train acc: 0.9992.. Val loss: 3.2931.. Val acc: 0.5713\n",
            "Epoch 171/300.. Learning rate: 0.0195.. Train loss: 0.0033.. Train acc: 0.9991.. Val loss: 3.2923.. Val acc: 0.5716\n",
            "Epoch 172/300.. Learning rate: 0.0193.. Train loss: 0.0028.. Train acc: 0.9994.. Val loss: 3.2919.. Val acc: 0.5702\n",
            "Epoch 173/300.. Learning rate: 0.0190.. Train loss: 0.0029.. Train acc: 0.9992.. Val loss: 3.2938.. Val acc: 0.5695\n",
            "Epoch 174/300.. Learning rate: 0.0188.. Train loss: 0.0026.. Train acc: 0.9993.. Val loss: 3.2655.. Val acc: 0.5706\n",
            "Epoch 175/300.. Learning rate: 0.0185.. Train loss: 0.0029.. Train acc: 0.9992.. Val loss: 3.2893.. Val acc: 0.5695\n",
            "Epoch 176/300.. Learning rate: 0.0183.. Train loss: 0.0023.. Train acc: 0.9996.. Val loss: 3.2807.. Val acc: 0.5678\n",
            "Epoch 177/300.. Learning rate: 0.0180.. Train loss: 0.0031.. Train acc: 0.9992.. Val loss: 3.2880.. Val acc: 0.5713\n",
            "Epoch 178/300.. Learning rate: 0.0178.. Train loss: 0.0028.. Train acc: 0.9992.. Val loss: 3.2712.. Val acc: 0.5715\n",
            "Epoch 179/300.. Learning rate: 0.0175.. Train loss: 0.0026.. Train acc: 0.9993.. Val loss: 3.2930.. Val acc: 0.5736\n",
            "Epoch 180/300.. Learning rate: 0.0173.. Train loss: 0.0026.. Train acc: 0.9994.. Val loss: 3.2842.. Val acc: 0.5711\n",
            "Epoch 181/300.. Learning rate: 0.0170.. Train loss: 0.0025.. Train acc: 0.9994.. Val loss: 3.2951.. Val acc: 0.5744\n",
            "Epoch 182/300.. Learning rate: 0.0168.. Train loss: 0.0023.. Train acc: 0.9995.. Val loss: 3.2798.. Val acc: 0.5711\n",
            "Epoch 183/300.. Learning rate: 0.0165.. Train loss: 0.0024.. Train acc: 0.9994.. Val loss: 3.3129.. Val acc: 0.5733\n",
            "Epoch 184/300.. Learning rate: 0.0163.. Train loss: 0.0025.. Train acc: 0.9993.. Val loss: 3.3002.. Val acc: 0.5742\n",
            "Epoch 185/300.. Learning rate: 0.0160.. Train loss: 0.0028.. Train acc: 0.9992.. Val loss: 3.2978.. Val acc: 0.5728\n",
            "Epoch 186/300.. Learning rate: 0.0158.. Train loss: 0.0024.. Train acc: 0.9994.. Val loss: 3.3006.. Val acc: 0.5703\n",
            "Epoch 187/300.. Learning rate: 0.0156.. Train loss: 0.0020.. Train acc: 0.9995.. Val loss: 3.2931.. Val acc: 0.5733\n",
            "Epoch 188/300.. Learning rate: 0.0153.. Train loss: 0.0021.. Train acc: 0.9994.. Val loss: 3.2958.. Val acc: 0.5710\n",
            "Epoch 189/300.. Learning rate: 0.0151.. Train loss: 0.0023.. Train acc: 0.9993.. Val loss: 3.2896.. Val acc: 0.5702\n",
            "Epoch 190/300.. Learning rate: 0.0148.. Train loss: 0.0022.. Train acc: 0.9994.. Val loss: 3.2849.. Val acc: 0.5730\n",
            "Epoch 191/300.. Learning rate: 0.0146.. Train loss: 0.0020.. Train acc: 0.9994.. Val loss: 3.3093.. Val acc: 0.5714\n",
            "Epoch 192/300.. Learning rate: 0.0144.. Train loss: 0.0022.. Train acc: 0.9992.. Val loss: 3.2971.. Val acc: 0.5739\n",
            "Epoch 193/300.. Learning rate: 0.0141.. Train loss: 0.0023.. Train acc: 0.9993.. Val loss: 3.2925.. Val acc: 0.5726\n",
            "Epoch 194/300.. Learning rate: 0.0139.. Train loss: 0.0021.. Train acc: 0.9993.. Val loss: 3.3024.. Val acc: 0.5742\n",
            "Epoch 195/300.. Learning rate: 0.0137.. Train loss: 0.0022.. Train acc: 0.9994.. Val loss: 3.3021.. Val acc: 0.5730\n",
            "Epoch 196/300.. Learning rate: 0.0134.. Train loss: 0.0020.. Train acc: 0.9994.. Val loss: 3.3141.. Val acc: 0.5716\n",
            "Epoch 197/300.. Learning rate: 0.0132.. Train loss: 0.0019.. Train acc: 0.9994.. Val loss: 3.3058.. Val acc: 0.5733\n",
            "Epoch 198/300.. Learning rate: 0.0130.. Train loss: 0.0019.. Train acc: 0.9995.. Val loss: 3.2747.. Val acc: 0.5737\n",
            "Epoch 199/300.. Learning rate: 0.0127.. Train loss: 0.0020.. Train acc: 0.9993.. Val loss: 3.3056.. Val acc: 0.5729\n",
            "Epoch 200/300.. Learning rate: 0.0125.. Train loss: 0.0021.. Train acc: 0.9994.. Val loss: 3.2902.. Val acc: 0.5727\n",
            "Epoch 201/300.. Learning rate: 0.0123.. Train loss: 0.0020.. Train acc: 0.9993.. Val loss: 3.2851.. Val acc: 0.5745\n",
            "Epoch 202/300.. Learning rate: 0.0121.. Train loss: 0.0020.. Train acc: 0.9995.. Val loss: 3.2822.. Val acc: 0.5724\n",
            "Epoch 203/300.. Learning rate: 0.0118.. Train loss: 0.0019.. Train acc: 0.9994.. Val loss: 3.2927.. Val acc: 0.5747\n",
            "Epoch 204/300.. Learning rate: 0.0116.. Train loss: 0.0019.. Train acc: 0.9994.. Val loss: 3.3064.. Val acc: 0.5735\n",
            "Epoch 205/300.. Learning rate: 0.0114.. Train loss: 0.0020.. Train acc: 0.9994.. Val loss: 3.2992.. Val acc: 0.5735\n",
            "Epoch 206/300.. Learning rate: 0.0112.. Train loss: 0.0021.. Train acc: 0.9993.. Val loss: 3.3091.. Val acc: 0.5717\n",
            "Epoch 207/300.. Learning rate: 0.0110.. Train loss: 0.0017.. Train acc: 0.9994.. Val loss: 3.2970.. Val acc: 0.5709\n",
            "Epoch 208/300.. Learning rate: 0.0107.. Train loss: 0.0019.. Train acc: 0.9994.. Val loss: 3.3057.. Val acc: 0.5746\n",
            "Epoch 209/300.. Learning rate: 0.0105.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3016.. Val acc: 0.5719\n",
            "Epoch 210/300.. Learning rate: 0.0103.. Train loss: 0.0020.. Train acc: 0.9994.. Val loss: 3.3144.. Val acc: 0.5743\n",
            "Epoch 211/300.. Learning rate: 0.0101.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.2949.. Val acc: 0.5728\n",
            "Epoch 212/300.. Learning rate: 0.0099.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.3064.. Val acc: 0.5738\n",
            "Epoch 213/300.. Learning rate: 0.0097.. Train loss: 0.0018.. Train acc: 0.9994.. Val loss: 3.2918.. Val acc: 0.5737\n",
            "Epoch 214/300.. Learning rate: 0.0095.. Train loss: 0.0018.. Train acc: 0.9995.. Val loss: 3.2934.. Val acc: 0.5741\n",
            "Epoch 215/300.. Learning rate: 0.0093.. Train loss: 0.0015.. Train acc: 0.9997.. Val loss: 3.2864.. Val acc: 0.5718\n",
            "Epoch 216/300.. Learning rate: 0.0091.. Train loss: 0.0020.. Train acc: 0.9993.. Val loss: 3.2963.. Val acc: 0.5761\n",
            "Epoch 217/300.. Learning rate: 0.0089.. Train loss: 0.0016.. Train acc: 0.9997.. Val loss: 3.2841.. Val acc: 0.5758\n",
            "Epoch 218/300.. Learning rate: 0.0087.. Train loss: 0.0022.. Train acc: 0.9992.. Val loss: 3.2943.. Val acc: 0.5766\n",
            "Epoch 219/300.. Learning rate: 0.0085.. Train loss: 0.0017.. Train acc: 0.9993.. Val loss: 3.2892.. Val acc: 0.5756\n",
            "Epoch 220/300.. Learning rate: 0.0083.. Train loss: 0.0016.. Train acc: 0.9997.. Val loss: 3.2972.. Val acc: 0.5742\n",
            "Epoch 221/300.. Learning rate: 0.0081.. Train loss: 0.0016.. Train acc: 0.9994.. Val loss: 3.2858.. Val acc: 0.5755\n",
            "Epoch 222/300.. Learning rate: 0.0079.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.2804.. Val acc: 0.5734\n",
            "Epoch 223/300.. Learning rate: 0.0077.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.3133.. Val acc: 0.5727\n",
            "Epoch 224/300.. Learning rate: 0.0075.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.2998.. Val acc: 0.5751\n",
            "Epoch 225/300.. Learning rate: 0.0073.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.2913.. Val acc: 0.5736\n",
            "Epoch 226/300.. Learning rate: 0.0071.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.2895.. Val acc: 0.5750\n",
            "Epoch 227/300.. Learning rate: 0.0070.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2900.. Val acc: 0.5748\n",
            "Epoch 228/300.. Learning rate: 0.0068.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3006.. Val acc: 0.5747\n",
            "Epoch 229/300.. Learning rate: 0.0066.. Train loss: 0.0017.. Train acc: 0.9994.. Val loss: 3.2943.. Val acc: 0.5749\n",
            "Epoch 230/300.. Learning rate: 0.0064.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.3028.. Val acc: 0.5744\n",
            "Epoch 231/300.. Learning rate: 0.0062.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.2977.. Val acc: 0.5747\n",
            "Epoch 232/300.. Learning rate: 0.0061.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.3081.. Val acc: 0.5749\n",
            "Epoch 233/300.. Learning rate: 0.0059.. Train loss: 0.0016.. Train acc: 0.9994.. Val loss: 3.2997.. Val acc: 0.5742\n",
            "Epoch 234/300.. Learning rate: 0.0057.. Train loss: 0.0015.. Train acc: 0.9997.. Val loss: 3.3061.. Val acc: 0.5752\n",
            "Epoch 235/300.. Learning rate: 0.0056.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.2996.. Val acc: 0.5735\n",
            "Epoch 236/300.. Learning rate: 0.0054.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.3097.. Val acc: 0.5739\n",
            "Epoch 237/300.. Learning rate: 0.0052.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.2958.. Val acc: 0.5738\n",
            "Epoch 238/300.. Learning rate: 0.0051.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3091.. Val acc: 0.5751\n",
            "Epoch 239/300.. Learning rate: 0.0049.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3062.. Val acc: 0.5749\n",
            "Epoch 240/300.. Learning rate: 0.0048.. Train loss: 0.0014.. Train acc: 0.9994.. Val loss: 3.2995.. Val acc: 0.5762\n",
            "Epoch 241/300.. Learning rate: 0.0046.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.2928.. Val acc: 0.5752\n",
            "Epoch 242/300.. Learning rate: 0.0045.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.2873.. Val acc: 0.5735\n",
            "Epoch 243/300.. Learning rate: 0.0043.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.2964.. Val acc: 0.5743\n",
            "Epoch 244/300.. Learning rate: 0.0042.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.2864.. Val acc: 0.5764\n",
            "Epoch 245/300.. Learning rate: 0.0040.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3026.. Val acc: 0.5749\n",
            "Epoch 246/300.. Learning rate: 0.0039.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3173.. Val acc: 0.5717\n",
            "Epoch 247/300.. Learning rate: 0.0038.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3053.. Val acc: 0.5749\n",
            "Epoch 248/300.. Learning rate: 0.0036.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3065.. Val acc: 0.5747\n",
            "Epoch 249/300.. Learning rate: 0.0035.. Train loss: 0.0015.. Train acc: 0.9994.. Val loss: 3.2896.. Val acc: 0.5744\n",
            "Epoch 250/300.. Learning rate: 0.0034.. Train loss: 0.0014.. Train acc: 0.9997.. Val loss: 3.3029.. Val acc: 0.5737\n",
            "Epoch 251/300.. Learning rate: 0.0032.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3016.. Val acc: 0.5754\n",
            "Epoch 252/300.. Learning rate: 0.0031.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3002.. Val acc: 0.5727\n",
            "Epoch 253/300.. Learning rate: 0.0030.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3008.. Val acc: 0.5735\n",
            "Epoch 254/300.. Learning rate: 0.0028.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3025.. Val acc: 0.5736\n",
            "Epoch 255/300.. Learning rate: 0.0027.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.3038.. Val acc: 0.5740\n",
            "Epoch 256/300.. Learning rate: 0.0026.. Train loss: 0.0017.. Train acc: 0.9994.. Val loss: 3.2973.. Val acc: 0.5766\n",
            "Epoch 257/300.. Learning rate: 0.0025.. Train loss: 0.0014.. Train acc: 0.9995.. Val loss: 3.2935.. Val acc: 0.5750\n",
            "Epoch 258/300.. Learning rate: 0.0024.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.3034.. Val acc: 0.5734\n",
            "Epoch 259/300.. Learning rate: 0.0023.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2953.. Val acc: 0.5737\n",
            "Epoch 260/300.. Learning rate: 0.0022.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.2838.. Val acc: 0.5762\n",
            "Epoch 261/300.. Learning rate: 0.0021.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.2904.. Val acc: 0.5749\n",
            "Epoch 262/300.. Learning rate: 0.0020.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.2810.. Val acc: 0.5746\n",
            "Epoch 263/300.. Learning rate: 0.0019.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3065.. Val acc: 0.5750\n",
            "Epoch 264/300.. Learning rate: 0.0018.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.2996.. Val acc: 0.5734\n",
            "Epoch 265/300.. Learning rate: 0.0017.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2920.. Val acc: 0.5753\n",
            "Epoch 266/300.. Learning rate: 0.0016.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.2964.. Val acc: 0.5749\n",
            "Epoch 267/300.. Learning rate: 0.0015.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.2943.. Val acc: 0.5761\n",
            "Epoch 268/300.. Learning rate: 0.0014.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.2902.. Val acc: 0.5752\n",
            "Epoch 269/300.. Learning rate: 0.0013.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.3039.. Val acc: 0.5772\n",
            "Epoch 270/300.. Learning rate: 0.0012.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.2881.. Val acc: 0.5759\n",
            "Epoch 271/300.. Learning rate: 0.0011.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3002.. Val acc: 0.5748\n",
            "Epoch 272/300.. Learning rate: 0.0011.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3029.. Val acc: 0.5740\n",
            "Epoch 273/300.. Learning rate: 0.0010.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2945.. Val acc: 0.5772\n",
            "Epoch 274/300.. Learning rate: 0.0009.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3172.. Val acc: 0.5747\n",
            "Epoch 275/300.. Learning rate: 0.0009.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.2955.. Val acc: 0.5764\n",
            "Epoch 276/300.. Learning rate: 0.0008.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3058.. Val acc: 0.5752\n",
            "Epoch 277/300.. Learning rate: 0.0007.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3110.. Val acc: 0.5740\n",
            "Epoch 278/300.. Learning rate: 0.0007.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.2960.. Val acc: 0.5747\n",
            "Epoch 279/300.. Learning rate: 0.0006.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.2975.. Val acc: 0.5750\n",
            "Epoch 280/300.. Learning rate: 0.0005.. Train loss: 0.0014.. Train acc: 0.9995.. Val loss: 3.3039.. Val acc: 0.5755\n",
            "Epoch 281/300.. Learning rate: 0.0005.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2975.. Val acc: 0.5758\n",
            "Epoch 282/300.. Learning rate: 0.0004.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3016.. Val acc: 0.5757\n",
            "Epoch 283/300.. Learning rate: 0.0004.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3006.. Val acc: 0.5743\n",
            "Epoch 284/300.. Learning rate: 0.0004.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.2903.. Val acc: 0.5752\n",
            "Epoch 285/300.. Learning rate: 0.0003.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2931.. Val acc: 0.5746\n",
            "Epoch 286/300.. Learning rate: 0.0003.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3036.. Val acc: 0.5727\n",
            "Epoch 287/300.. Learning rate: 0.0002.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3056.. Val acc: 0.5728\n",
            "Epoch 288/300.. Learning rate: 0.0002.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2915.. Val acc: 0.5764\n",
            "Epoch 289/300.. Learning rate: 0.0002.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3016.. Val acc: 0.5746\n",
            "Epoch 290/300.. Learning rate: 0.0001.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3011.. Val acc: 0.5757\n",
            "Epoch 291/300.. Learning rate: 0.0001.. Train loss: 0.0014.. Train acc: 0.9995.. Val loss: 3.2904.. Val acc: 0.5759\n",
            "Epoch 292/300.. Learning rate: 0.0001.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3039.. Val acc: 0.5737\n",
            "Epoch 293/300.. Learning rate: 0.0001.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3020.. Val acc: 0.5756\n",
            "Epoch 294/300.. Learning rate: 0.0001.. Train loss: 0.0013.. Train acc: 0.9995.. Val loss: 3.2968.. Val acc: 0.5750\n",
            "Epoch 295/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3041.. Val acc: 0.5749\n",
            "Epoch 296/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.3013.. Val acc: 0.5764\n",
            "Epoch 297/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.2924.. Val acc: 0.5766\n",
            "Epoch 298/300.. Learning rate: 0.0000.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.2997.. Val acc: 0.5740\n",
            "Epoch 299/300.. Learning rate: 0.0000.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.2861.. Val acc: 0.5749\n",
            "Epoch 300/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3018.. Val acc: 0.5759\n",
            "Image to be saved: lr=0.05_epochs=300_lr_sch_cos57.png\n",
            "Test loss:  3.3305955631256103\n",
            "Test acc:  0.5767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WEIGHT DECAY"
      ],
      "metadata": {
        "id": "VnVHNEqUUKqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wd with gamma=0.0005"
      ],
      "metadata": {
        "id": "0GvY2SF-USDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 300 \\\n",
        "--lr 0.05 \\\n",
        "--wd 0.0005\\\n",
        "--lr_scheduler \\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.05_epochs=300_wd=0.0005.png \\\n",
        "--test\n"
      ],
      "metadata": {
        "id": "ALbE3iwtUW_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1d7b98-60ac-44de-de8b-fec63e6c8668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean: [0.507108747959137, 0.48638272285461426, 0.4406910240650177], Std: [0.2675017714500427, 0.2566560208797455, 0.2763121724128723]\n",
            "ARGS PASSED:  Namespace(dataset_dir='./mydata2/', batch_size=128, epochs=300, lr=0.05, wd=0.0005, fig_name='lr=0.05_epochs=300.png', lr_scheduler=True, mixup=False, alpha=1.0, test=True, save_images=False, seed=0)\n",
            "DIR ----> DIAGRAM not created ---->[Errno 17] File exists: '/content/diagram'\n",
            "DATASET DIR:  mydata2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1859.. Train acc: 0.0602.. Val loss: 3.8393.. Val acc: 0.1003\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6727.. Train acc: 0.1244.. Val loss: 3.4193.. Val acc: 0.1625\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.3655.. Train acc: 0.1739.. Val loss: 3.2417.. Val acc: 0.1964\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.1085.. Train acc: 0.2214.. Val loss: 3.0323.. Val acc: 0.2503\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.8684.. Train acc: 0.2699.. Val loss: 2.7685.. Val acc: 0.2904\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.6787.. Train acc: 0.3023.. Val loss: 2.6280.. Val acc: 0.3195\n",
            "Epoch 7/300.. Learning rate: 0.0499.. Train loss: 2.5211.. Train acc: 0.3371.. Val loss: 2.6710.. Val acc: 0.3106\n",
            "Epoch 8/300.. Learning rate: 0.0499.. Train loss: 2.3959.. Train acc: 0.3597.. Val loss: 2.4512.. Val acc: 0.3564\n",
            "Epoch 9/300.. Learning rate: 0.0499.. Train loss: 2.2650.. Train acc: 0.3907.. Val loss: 2.4683.. Val acc: 0.3575\n",
            "Epoch 10/300.. Learning rate: 0.0499.. Train loss: 2.1736.. Train acc: 0.4097.. Val loss: 2.3073.. Val acc: 0.3896\n",
            "Epoch 11/300.. Learning rate: 0.0498.. Train loss: 2.0747.. Train acc: 0.4333.. Val loss: 2.3124.. Val acc: 0.3852\n",
            "Epoch 12/300.. Learning rate: 0.0498.. Train loss: 2.0148.. Train acc: 0.4479.. Val loss: 2.1287.. Val acc: 0.4298\n",
            "Epoch 13/300.. Learning rate: 0.0498.. Train loss: 1.9353.. Train acc: 0.4667.. Val loss: 2.0387.. Val acc: 0.4419\n",
            "Epoch 14/300.. Learning rate: 0.0497.. Train loss: 1.8848.. Train acc: 0.4782.. Val loss: 2.1912.. Val acc: 0.4189\n",
            "Epoch 15/300.. Learning rate: 0.0497.. Train loss: 1.8377.. Train acc: 0.4908.. Val loss: 2.0022.. Val acc: 0.4584\n",
            "Epoch 16/300.. Learning rate: 0.0497.. Train loss: 1.7957.. Train acc: 0.4996.. Val loss: 2.1007.. Val acc: 0.4437\n",
            "Epoch 17/300.. Learning rate: 0.0496.. Train loss: 1.7544.. Train acc: 0.5080.. Val loss: 1.9642.. Val acc: 0.4668\n",
            "Epoch 18/300.. Learning rate: 0.0496.. Train loss: 1.7219.. Train acc: 0.5165.. Val loss: 1.9533.. Val acc: 0.4605\n",
            "Epoch 19/300.. Learning rate: 0.0495.. Train loss: 1.6791.. Train acc: 0.5279.. Val loss: 2.0293.. Val acc: 0.4594\n",
            "Epoch 20/300.. Learning rate: 0.0495.. Train loss: 1.6487.. Train acc: 0.5363.. Val loss: 1.9660.. Val acc: 0.4678\n",
            "Epoch 21/300.. Learning rate: 0.0494.. Train loss: 1.6233.. Train acc: 0.5376.. Val loss: 1.9626.. Val acc: 0.4772\n",
            "Epoch 22/300.. Learning rate: 0.0493.. Train loss: 1.6055.. Train acc: 0.5465.. Val loss: 2.0174.. Val acc: 0.4655\n",
            "Epoch 23/300.. Learning rate: 0.0493.. Train loss: 1.5771.. Train acc: 0.5522.. Val loss: 1.8668.. Val acc: 0.4862\n",
            "Epoch 24/300.. Learning rate: 0.0492.. Train loss: 1.5586.. Train acc: 0.5571.. Val loss: 1.8772.. Val acc: 0.4895\n",
            "Epoch 25/300.. Learning rate: 0.0492.. Train loss: 1.5413.. Train acc: 0.5601.. Val loss: 1.9951.. Val acc: 0.4700\n",
            "Epoch 26/300.. Learning rate: 0.0491.. Train loss: 1.5246.. Train acc: 0.5653.. Val loss: 1.8444.. Val acc: 0.5020\n",
            "Epoch 27/300.. Learning rate: 0.0490.. Train loss: 1.5055.. Train acc: 0.5723.. Val loss: 2.0354.. Val acc: 0.4682\n",
            "Epoch 28/300.. Learning rate: 0.0489.. Train loss: 1.4797.. Train acc: 0.5735.. Val loss: 1.8995.. Val acc: 0.4934\n",
            "Epoch 29/300.. Learning rate: 0.0489.. Train loss: 1.4765.. Train acc: 0.5776.. Val loss: 1.9009.. Val acc: 0.4858\n",
            "Epoch 30/300.. Learning rate: 0.0488.. Train loss: 1.4554.. Train acc: 0.5845.. Val loss: 1.7552.. Val acc: 0.5255\n",
            "Epoch 31/300.. Learning rate: 0.0487.. Train loss: 1.4492.. Train acc: 0.5861.. Val loss: 1.7908.. Val acc: 0.5157\n",
            "Epoch 32/300.. Learning rate: 0.0486.. Train loss: 1.4488.. Train acc: 0.5838.. Val loss: 1.8198.. Val acc: 0.5090\n",
            "Epoch 33/300.. Learning rate: 0.0485.. Train loss: 1.4209.. Train acc: 0.5896.. Val loss: 1.7966.. Val acc: 0.5202\n",
            "Epoch 34/300.. Learning rate: 0.0484.. Train loss: 1.4088.. Train acc: 0.5936.. Val loss: 1.9018.. Val acc: 0.4915\n",
            "Epoch 35/300.. Learning rate: 0.0483.. Train loss: 1.4035.. Train acc: 0.5929.. Val loss: 1.8187.. Val acc: 0.5119\n",
            "Epoch 36/300.. Learning rate: 0.0482.. Train loss: 1.3820.. Train acc: 0.5979.. Val loss: 1.8396.. Val acc: 0.5060\n",
            "Epoch 37/300.. Learning rate: 0.0481.. Train loss: 1.3750.. Train acc: 0.6009.. Val loss: 1.8503.. Val acc: 0.5126\n",
            "Epoch 38/300.. Learning rate: 0.0480.. Train loss: 1.3663.. Train acc: 0.6028.. Val loss: 1.8532.. Val acc: 0.5121\n",
            "Epoch 39/300.. Learning rate: 0.0479.. Train loss: 1.3512.. Train acc: 0.6063.. Val loss: 1.8305.. Val acc: 0.5171\n",
            "Epoch 40/300.. Learning rate: 0.0478.. Train loss: 1.3599.. Train acc: 0.6058.. Val loss: 1.9333.. Val acc: 0.4982\n",
            "Epoch 41/300.. Learning rate: 0.0477.. Train loss: 1.3347.. Train acc: 0.6139.. Val loss: 1.7733.. Val acc: 0.5217\n",
            "Epoch 42/300.. Learning rate: 0.0476.. Train loss: 1.3338.. Train acc: 0.6126.. Val loss: 1.8201.. Val acc: 0.5211\n",
            "Epoch 43/300.. Learning rate: 0.0475.. Train loss: 1.3273.. Train acc: 0.6132.. Val loss: 1.8306.. Val acc: 0.5161\n",
            "Epoch 44/300.. Learning rate: 0.0474.. Train loss: 1.3198.. Train acc: 0.6150.. Val loss: 1.7697.. Val acc: 0.5246\n",
            "Epoch 45/300.. Learning rate: 0.0473.. Train loss: 1.3092.. Train acc: 0.6197.. Val loss: 1.8362.. Val acc: 0.5135\n",
            "Epoch 46/300.. Learning rate: 0.0472.. Train loss: 1.2951.. Train acc: 0.6220.. Val loss: 1.7507.. Val acc: 0.5331\n",
            "Epoch 47/300.. Learning rate: 0.0470.. Train loss: 1.2879.. Train acc: 0.6235.. Val loss: 1.6945.. Val acc: 0.5447\n",
            "Epoch 48/300.. Learning rate: 0.0469.. Train loss: 1.2834.. Train acc: 0.6253.. Val loss: 1.7714.. Val acc: 0.5236\n",
            "Epoch 49/300.. Learning rate: 0.0468.. Train loss: 1.2652.. Train acc: 0.6304.. Val loss: 1.9587.. Val acc: 0.4951\n",
            "Epoch 50/300.. Learning rate: 0.0467.. Train loss: 1.2665.. Train acc: 0.6308.. Val loss: 1.8295.. Val acc: 0.5173\n",
            "Epoch 51/300.. Learning rate: 0.0465.. Train loss: 1.2514.. Train acc: 0.6343.. Val loss: 1.8358.. Val acc: 0.5214\n",
            "Epoch 52/300.. Learning rate: 0.0464.. Train loss: 1.2442.. Train acc: 0.6366.. Val loss: 1.6956.. Val acc: 0.5462\n",
            "Epoch 53/300.. Learning rate: 0.0463.. Train loss: 1.2542.. Train acc: 0.6299.. Val loss: 1.6929.. Val acc: 0.5506\n",
            "Epoch 54/300.. Learning rate: 0.0461.. Train loss: 1.2362.. Train acc: 0.6379.. Val loss: 1.6634.. Val acc: 0.5484\n",
            "Epoch 55/300.. Learning rate: 0.0460.. Train loss: 1.2290.. Train acc: 0.6406.. Val loss: 1.7703.. Val acc: 0.5344\n",
            "Epoch 56/300.. Learning rate: 0.0458.. Train loss: 1.2097.. Train acc: 0.6445.. Val loss: 1.8613.. Val acc: 0.5103\n",
            "Epoch 57/300.. Learning rate: 0.0457.. Train loss: 1.2194.. Train acc: 0.6417.. Val loss: 1.7274.. Val acc: 0.5413\n",
            "Epoch 58/300.. Learning rate: 0.0455.. Train loss: 1.2169.. Train acc: 0.6431.. Val loss: 1.7655.. Val acc: 0.5281\n",
            "Epoch 59/300.. Learning rate: 0.0454.. Train loss: 1.1991.. Train acc: 0.6476.. Val loss: 1.7148.. Val acc: 0.5432\n",
            "Epoch 60/300.. Learning rate: 0.0452.. Train loss: 1.1938.. Train acc: 0.6495.. Val loss: 1.8076.. Val acc: 0.5227\n",
            "Epoch 61/300.. Learning rate: 0.0451.. Train loss: 1.2025.. Train acc: 0.6469.. Val loss: 1.7304.. Val acc: 0.5383\n",
            "Epoch 62/300.. Learning rate: 0.0449.. Train loss: 1.1867.. Train acc: 0.6496.. Val loss: 1.6789.. Val acc: 0.5487\n",
            "Epoch 63/300.. Learning rate: 0.0448.. Train loss: 1.1746.. Train acc: 0.6523.. Val loss: 1.7155.. Val acc: 0.5437\n",
            "Epoch 64/300.. Learning rate: 0.0446.. Train loss: 1.1760.. Train acc: 0.6520.. Val loss: 1.7812.. Val acc: 0.5341\n",
            "Epoch 65/300.. Learning rate: 0.0444.. Train loss: 1.1718.. Train acc: 0.6540.. Val loss: 1.7049.. Val acc: 0.5397\n",
            "Epoch 66/300.. Learning rate: 0.0443.. Train loss: 1.1619.. Train acc: 0.6561.. Val loss: 1.7335.. Val acc: 0.5412\n",
            "Epoch 67/300.. Learning rate: 0.0441.. Train loss: 1.1597.. Train acc: 0.6565.. Val loss: 1.6885.. Val acc: 0.5492\n",
            "Epoch 68/300.. Learning rate: 0.0439.. Train loss: 1.1527.. Train acc: 0.6582.. Val loss: 1.7719.. Val acc: 0.5323\n",
            "Epoch 69/300.. Learning rate: 0.0438.. Train loss: 1.1454.. Train acc: 0.6611.. Val loss: 1.7898.. Val acc: 0.5320\n",
            "Epoch 70/300.. Learning rate: 0.0436.. Train loss: 1.1314.. Train acc: 0.6645.. Val loss: 1.7505.. Val acc: 0.5327\n",
            "Epoch 71/300.. Learning rate: 0.0434.. Train loss: 1.1346.. Train acc: 0.6640.. Val loss: 1.7301.. Val acc: 0.5466\n",
            "Epoch 72/300.. Learning rate: 0.0432.. Train loss: 1.1227.. Train acc: 0.6685.. Val loss: 1.7667.. Val acc: 0.5356\n",
            "Epoch 73/300.. Learning rate: 0.0430.. Train loss: 1.1331.. Train acc: 0.6669.. Val loss: 1.6356.. Val acc: 0.5596\n",
            "Epoch 74/300.. Learning rate: 0.0429.. Train loss: 1.1055.. Train acc: 0.6724.. Val loss: 1.7951.. Val acc: 0.5396\n",
            "Epoch 75/300.. Learning rate: 0.0427.. Train loss: 1.1105.. Train acc: 0.6721.. Val loss: 1.6998.. Val acc: 0.5495\n",
            "Epoch 76/300.. Learning rate: 0.0425.. Train loss: 1.0960.. Train acc: 0.6750.. Val loss: 1.6944.. Val acc: 0.5587\n",
            "Epoch 77/300.. Learning rate: 0.0423.. Train loss: 1.0830.. Train acc: 0.6754.. Val loss: 1.6922.. Val acc: 0.5556\n",
            "Epoch 78/300.. Learning rate: 0.0421.. Train loss: 1.0951.. Train acc: 0.6741.. Val loss: 1.6850.. Val acc: 0.5565\n",
            "Epoch 79/300.. Learning rate: 0.0419.. Train loss: 1.0790.. Train acc: 0.6830.. Val loss: 1.6768.. Val acc: 0.5592\n",
            "Epoch 80/300.. Learning rate: 0.0417.. Train loss: 1.0841.. Train acc: 0.6800.. Val loss: 1.7147.. Val acc: 0.5492\n",
            "Epoch 81/300.. Learning rate: 0.0415.. Train loss: 1.0930.. Train acc: 0.6745.. Val loss: 1.7312.. Val acc: 0.5498\n",
            "Epoch 82/300.. Learning rate: 0.0413.. Train loss: 1.0763.. Train acc: 0.6811.. Val loss: 1.7086.. Val acc: 0.5510\n",
            "Epoch 83/300.. Learning rate: 0.0411.. Train loss: 1.0718.. Train acc: 0.6803.. Val loss: 1.6069.. Val acc: 0.5681\n",
            "Epoch 84/300.. Learning rate: 0.0409.. Train loss: 1.0761.. Train acc: 0.6811.. Val loss: 1.7339.. Val acc: 0.5489\n",
            "Epoch 85/300.. Learning rate: 0.0407.. Train loss: 1.0655.. Train acc: 0.6836.. Val loss: 1.6698.. Val acc: 0.5579\n",
            "Epoch 86/300.. Learning rate: 0.0405.. Train loss: 1.0484.. Train acc: 0.6858.. Val loss: 1.6397.. Val acc: 0.5642\n",
            "Epoch 87/300.. Learning rate: 0.0403.. Train loss: 1.0534.. Train acc: 0.6861.. Val loss: 1.7644.. Val acc: 0.5408\n",
            "Epoch 88/300.. Learning rate: 0.0401.. Train loss: 1.0508.. Train acc: 0.6871.. Val loss: 1.6049.. Val acc: 0.5754\n",
            "Epoch 89/300.. Learning rate: 0.0399.. Train loss: 1.0337.. Train acc: 0.6921.. Val loss: 1.6405.. Val acc: 0.5663\n",
            "Epoch 90/300.. Learning rate: 0.0397.. Train loss: 1.0412.. Train acc: 0.6904.. Val loss: 1.6266.. Val acc: 0.5662\n",
            "Epoch 91/300.. Learning rate: 0.0395.. Train loss: 1.0322.. Train acc: 0.6893.. Val loss: 1.6639.. Val acc: 0.5547\n",
            "Epoch 92/300.. Learning rate: 0.0393.. Train loss: 1.0347.. Train acc: 0.6880.. Val loss: 1.6998.. Val acc: 0.5588\n",
            "Epoch 93/300.. Learning rate: 0.0391.. Train loss: 1.0194.. Train acc: 0.6956.. Val loss: 1.7002.. Val acc: 0.5508\n",
            "Epoch 94/300.. Learning rate: 0.0388.. Train loss: 1.0067.. Train acc: 0.6993.. Val loss: 1.8028.. Val acc: 0.5388\n",
            "Epoch 95/300.. Learning rate: 0.0386.. Train loss: 1.0042.. Train acc: 0.6986.. Val loss: 1.6584.. Val acc: 0.5658\n",
            "Epoch 96/300.. Learning rate: 0.0384.. Train loss: 0.9954.. Train acc: 0.7023.. Val loss: 1.5862.. Val acc: 0.5775\n",
            "Epoch 97/300.. Learning rate: 0.0382.. Train loss: 0.9923.. Train acc: 0.7030.. Val loss: 1.6632.. Val acc: 0.5715\n",
            "Epoch 98/300.. Learning rate: 0.0380.. Train loss: 0.9881.. Train acc: 0.7035.. Val loss: 1.6806.. Val acc: 0.5652\n",
            "Epoch 99/300.. Learning rate: 0.0377.. Train loss: 0.9933.. Train acc: 0.7009.. Val loss: 1.6187.. Val acc: 0.5716\n",
            "Epoch 100/300.. Learning rate: 0.0375.. Train loss: 0.9794.. Train acc: 0.7041.. Val loss: 1.7285.. Val acc: 0.5448\n",
            "Epoch 101/300.. Learning rate: 0.0373.. Train loss: 0.9832.. Train acc: 0.7048.. Val loss: 1.5809.. Val acc: 0.5844\n",
            "Epoch 102/300.. Learning rate: 0.0370.. Train loss: 0.9794.. Train acc: 0.7062.. Val loss: 1.6992.. Val acc: 0.5549\n",
            "Epoch 103/300.. Learning rate: 0.0368.. Train loss: 0.9716.. Train acc: 0.7048.. Val loss: 1.7247.. Val acc: 0.5546\n",
            "Epoch 104/300.. Learning rate: 0.0366.. Train loss: 0.9554.. Train acc: 0.7096.. Val loss: 1.7041.. Val acc: 0.5635\n",
            "Epoch 105/300.. Learning rate: 0.0364.. Train loss: 0.9623.. Train acc: 0.7087.. Val loss: 1.6745.. Val acc: 0.5615\n",
            "Epoch 106/300.. Learning rate: 0.0361.. Train loss: 0.9549.. Train acc: 0.7119.. Val loss: 1.6943.. Val acc: 0.5573\n",
            "Epoch 107/300.. Learning rate: 0.0359.. Train loss: 0.9501.. Train acc: 0.7124.. Val loss: 1.6244.. Val acc: 0.5778\n",
            "Epoch 108/300.. Learning rate: 0.0356.. Train loss: 0.9328.. Train acc: 0.7179.. Val loss: 1.7429.. Val acc: 0.5568\n",
            "Epoch 109/300.. Learning rate: 0.0354.. Train loss: 0.9431.. Train acc: 0.7174.. Val loss: 1.5933.. Val acc: 0.5789\n",
            "Epoch 110/300.. Learning rate: 0.0352.. Train loss: 0.9291.. Train acc: 0.7187.. Val loss: 1.7598.. Val acc: 0.5475\n",
            "Epoch 111/300.. Learning rate: 0.0349.. Train loss: 0.9227.. Train acc: 0.7201.. Val loss: 1.6136.. Val acc: 0.5725\n",
            "Epoch 112/300.. Learning rate: 0.0347.. Train loss: 0.9203.. Train acc: 0.7210.. Val loss: 1.6453.. Val acc: 0.5746\n",
            "Epoch 113/300.. Learning rate: 0.0344.. Train loss: 0.9135.. Train acc: 0.7220.. Val loss: 1.5629.. Val acc: 0.5877\n",
            "Epoch 114/300.. Learning rate: 0.0342.. Train loss: 0.9074.. Train acc: 0.7262.. Val loss: 1.6198.. Val acc: 0.5764\n",
            "Epoch 115/300.. Learning rate: 0.0340.. Train loss: 0.9052.. Train acc: 0.7252.. Val loss: 1.5825.. Val acc: 0.5801\n",
            "Epoch 116/300.. Learning rate: 0.0337.. Train loss: 0.8928.. Train acc: 0.7309.. Val loss: 1.6659.. Val acc: 0.5641\n",
            "Epoch 117/300.. Learning rate: 0.0335.. Train loss: 0.8795.. Train acc: 0.7337.. Val loss: 1.6455.. Val acc: 0.5680\n",
            "Epoch 118/300.. Learning rate: 0.0332.. Train loss: 0.8864.. Train acc: 0.7309.. Val loss: 1.6904.. Val acc: 0.5662\n",
            "Epoch 119/300.. Learning rate: 0.0330.. Train loss: 0.8829.. Train acc: 0.7309.. Val loss: 1.6746.. Val acc: 0.5664\n",
            "Epoch 120/300.. Learning rate: 0.0327.. Train loss: 0.8613.. Train acc: 0.7362.. Val loss: 1.7438.. Val acc: 0.5577\n",
            "Epoch 122/300.. Learning rate: 0.0322.. Train loss: 0.8610.. Train acc: 0.7372.. Val loss: 1.6453.. Val acc: 0.5749\n",
            "Epoch 123/300.. Learning rate: 0.0320.. Train loss: 0.8433.. Train acc: 0.7400.. Val loss: 1.6933.. Val acc: 0.5638\n",
            "Epoch 124/300.. Learning rate: 0.0317.. Train loss: 0.8448.. Train acc: 0.7425.. Val loss: 1.6322.. Val acc: 0.5747\n",
            "Epoch 125/300.. Learning rate: 0.0315.. Train loss: 0.8338.. Train acc: 0.7459.. Val loss: 1.6382.. Val acc: 0.5785\n",
            "Epoch 126/300.. Learning rate: 0.0312.. Train loss: 0.8426.. Train acc: 0.7424.. Val loss: 1.6557.. Val acc: 0.5729\n",
            "Epoch 127/300.. Learning rate: 0.0310.. Train loss: 0.8291.. Train acc: 0.7452.. Val loss: 1.7265.. Val acc: 0.5648\n",
            "Epoch 128/300.. Learning rate: 0.0307.. Train loss: 0.8235.. Train acc: 0.7477.. Val loss: 1.6825.. Val acc: 0.5749\n",
            "Epoch 129/300.. Learning rate: 0.0305.. Train loss: 0.8120.. Train acc: 0.7521.. Val loss: 1.6151.. Val acc: 0.5837\n",
            "Epoch 130/300.. Learning rate: 0.0302.. Train loss: 0.8096.. Train acc: 0.7530.. Val loss: 1.6091.. Val acc: 0.5930\n",
            "Epoch 131/300.. Learning rate: 0.0299.. Train loss: 0.7955.. Train acc: 0.7558.. Val loss: 1.7425.. Val acc: 0.5566\n",
            "Epoch 132/300.. Learning rate: 0.0297.. Train loss: 0.7937.. Train acc: 0.7562.. Val loss: 1.5669.. Val acc: 0.5955\n",
            "Epoch 133/300.. Learning rate: 0.0294.. Train loss: 0.7849.. Train acc: 0.7608.. Val loss: 1.6203.. Val acc: 0.5852\n",
            "Epoch 134/300.. Learning rate: 0.0292.. Train loss: 0.7800.. Train acc: 0.7587.. Val loss: 1.7771.. Val acc: 0.5550\n",
            "Epoch 135/300.. Learning rate: 0.0289.. Train loss: 0.7814.. Train acc: 0.7574.. Val loss: 1.6426.. Val acc: 0.5828\n",
            "Epoch 136/300.. Learning rate: 0.0287.. Train loss: 0.7648.. Train acc: 0.7662.. Val loss: 1.6469.. Val acc: 0.5763\n",
            "Epoch 137/300.. Learning rate: 0.0284.. Train loss: 0.7586.. Train acc: 0.7672.. Val loss: 1.6177.. Val acc: 0.5851\n",
            "Epoch 138/300.. Learning rate: 0.0281.. Train loss: 0.7320.. Train acc: 0.7733.. Val loss: 1.6510.. Val acc: 0.5813\n",
            "Epoch 139/300.. Learning rate: 0.0279.. Train loss: 0.7519.. Train acc: 0.7686.. Val loss: 1.6108.. Val acc: 0.5870\n",
            "Epoch 140/300.. Learning rate: 0.0276.. Train loss: 0.7416.. Train acc: 0.7722.. Val loss: 1.7589.. Val acc: 0.5671\n",
            "Epoch 141/300.. Learning rate: 0.0274.. Train loss: 0.7209.. Train acc: 0.7783.. Val loss: 1.6289.. Val acc: 0.5860\n",
            "Epoch 142/300.. Learning rate: 0.0271.. Train loss: 0.7276.. Train acc: 0.7733.. Val loss: 1.5895.. Val acc: 0.5971\n",
            "Epoch 143/300.. Learning rate: 0.0268.. Train loss: 0.7101.. Train acc: 0.7789.. Val loss: 1.5721.. Val acc: 0.5994\n",
            "Epoch 144/300.. Learning rate: 0.0266.. Train loss: 0.7114.. Train acc: 0.7801.. Val loss: 1.5929.. Val acc: 0.5904\n",
            "Epoch 145/300.. Learning rate: 0.0263.. Train loss: 0.6956.. Train acc: 0.7847.. Val loss: 1.6208.. Val acc: 0.5906\n",
            "Epoch 146/300.. Learning rate: 0.0260.. Train loss: 0.6823.. Train acc: 0.7893.. Val loss: 1.6619.. Val acc: 0.5833\n",
            "Epoch 147/300.. Learning rate: 0.0258.. Train loss: 0.6741.. Train acc: 0.7906.. Val loss: 1.6060.. Val acc: 0.5944\n",
            "Epoch 148/300.. Learning rate: 0.0255.. Train loss: 0.6622.. Train acc: 0.7936.. Val loss: 1.6447.. Val acc: 0.5906\n",
            "Epoch 149/300.. Learning rate: 0.0253.. Train loss: 0.6751.. Train acc: 0.7893.. Val loss: 1.5978.. Val acc: 0.5939\n",
            "Epoch 150/300.. Learning rate: 0.0250.. Train loss: 0.6546.. Train acc: 0.7984.. Val loss: 1.6364.. Val acc: 0.5896\n",
            "Epoch 151/300.. Learning rate: 0.0247.. Train loss: 0.6543.. Train acc: 0.7946.. Val loss: 1.6108.. Val acc: 0.5870\n",
            "Epoch 152/300.. Learning rate: 0.0245.. Train loss: 0.6463.. Train acc: 0.7977.. Val loss: 1.7049.. Val acc: 0.5777\n",
            "Epoch 153/300.. Learning rate: 0.0242.. Train loss: 0.6375.. Train acc: 0.8008.. Val loss: 1.7221.. Val acc: 0.5767\n",
            "Epoch 154/300.. Learning rate: 0.0240.. Train loss: 0.6196.. Train acc: 0.8081.. Val loss: 1.6389.. Val acc: 0.5883\n",
            "Epoch 155/300.. Learning rate: 0.0237.. Train loss: 0.6251.. Train acc: 0.8036.. Val loss: 1.5675.. Val acc: 0.6054\n",
            "Epoch 156/300.. Learning rate: 0.0234.. Train loss: 0.6036.. Train acc: 0.8107.. Val loss: 1.5542.. Val acc: 0.6110\n",
            "Epoch 157/300.. Learning rate: 0.0232.. Train loss: 0.5886.. Train acc: 0.8165.. Val loss: 1.6307.. Val acc: 0.5972\n",
            "Epoch 158/300.. Learning rate: 0.0229.. Train loss: 0.6005.. Train acc: 0.8112.. Val loss: 1.6235.. Val acc: 0.5970\n",
            "Epoch 159/300.. Learning rate: 0.0227.. Train loss: 0.6021.. Train acc: 0.8110.. Val loss: 1.7046.. Val acc: 0.5795\n",
            "Epoch 160/300.. Learning rate: 0.0224.. Train loss: 0.5748.. Train acc: 0.8198.. Val loss: 1.6074.. Val acc: 0.5972\n",
            "Epoch 161/300.. Learning rate: 0.0221.. Train loss: 0.5732.. Train acc: 0.8209.. Val loss: 1.6381.. Val acc: 0.5964\n",
            "Epoch 162/300.. Learning rate: 0.0219.. Train loss: 0.5660.. Train acc: 0.8205.. Val loss: 1.6272.. Val acc: 0.5944\n",
            "Epoch 163/300.. Learning rate: 0.0216.. Train loss: 0.5691.. Train acc: 0.8209.. Val loss: 1.6049.. Val acc: 0.6021\n",
            "Epoch 164/300.. Learning rate: 0.0214.. Train loss: 0.5424.. Train acc: 0.8294.. Val loss: 1.6655.. Val acc: 0.5939\n",
            "Epoch 165/300.. Learning rate: 0.0211.. Train loss: 0.5342.. Train acc: 0.8311.. Val loss: 1.6177.. Val acc: 0.6016\n",
            "Epoch 166/300.. Learning rate: 0.0208.. Train loss: 0.5297.. Train acc: 0.8333.. Val loss: 1.6441.. Val acc: 0.5979\n",
            "Epoch 167/300.. Learning rate: 0.0206.. Train loss: 0.5282.. Train acc: 0.8325.. Val loss: 1.6284.. Val acc: 0.6011\n",
            "Epoch 168/300.. Learning rate: 0.0203.. Train loss: 0.5221.. Train acc: 0.8355.. Val loss: 1.6228.. Val acc: 0.5991\n",
            "Epoch 169/300.. Learning rate: 0.0201.. Train loss: 0.5075.. Train acc: 0.8417.. Val loss: 1.5767.. Val acc: 0.6077\n",
            "Epoch 170/300.. Learning rate: 0.0198.. Train loss: 0.4915.. Train acc: 0.8455.. Val loss: 1.6490.. Val acc: 0.6011\n",
            "Epoch 171/300.. Learning rate: 0.0195.. Train loss: 0.4845.. Train acc: 0.8478.. Val loss: 1.6918.. Val acc: 0.5912\n",
            "Epoch 172/300.. Learning rate: 0.0193.. Train loss: 0.4985.. Train acc: 0.8429.. Val loss: 1.7054.. Val acc: 0.5892\n",
            "Epoch 173/300.. Learning rate: 0.0190.. Train loss: 0.4891.. Train acc: 0.8461.. Val loss: 1.6376.. Val acc: 0.6031\n",
            "Epoch 174/300.. Learning rate: 0.0188.. Train loss: 0.4689.. Train acc: 0.8497.. Val loss: 1.5786.. Val acc: 0.6121\n",
            "Epoch 175/300.. Learning rate: 0.0185.. Train loss: 0.4610.. Train acc: 0.8545.. Val loss: 1.6788.. Val acc: 0.5986\n",
            "Epoch 176/300.. Learning rate: 0.0183.. Train loss: 0.4546.. Train acc: 0.8563.. Val loss: 1.6471.. Val acc: 0.5990\n",
            "Epoch 177/300.. Learning rate: 0.0180.. Train loss: 0.4468.. Train acc: 0.8596.. Val loss: 1.6408.. Val acc: 0.6093\n",
            "Epoch 178/300.. Learning rate: 0.0178.. Train loss: 0.4346.. Train acc: 0.8633.. Val loss: 1.6256.. Val acc: 0.6075\n",
            "Epoch 179/300.. Learning rate: 0.0175.. Train loss: 0.4288.. Train acc: 0.8638.. Val loss: 1.6370.. Val acc: 0.6129\n",
            "Epoch 180/300.. Learning rate: 0.0173.. Train loss: 0.4258.. Train acc: 0.8669.. Val loss: 1.6161.. Val acc: 0.6131\n",
            "Epoch 181/300.. Learning rate: 0.0170.. Train loss: 0.4138.. Train acc: 0.8701.. Val loss: 1.7013.. Val acc: 0.6000\n",
            "Epoch 182/300.. Learning rate: 0.0168.. Train loss: 0.4075.. Train acc: 0.8722.. Val loss: 1.6247.. Val acc: 0.6060\n",
            "Epoch 183/300.. Learning rate: 0.0165.. Train loss: 0.3928.. Train acc: 0.8763.. Val loss: 1.6289.. Val acc: 0.6115\n",
            "Epoch 184/300.. Learning rate: 0.0163.. Train loss: 0.3891.. Train acc: 0.8778.. Val loss: 1.6152.. Val acc: 0.6135\n",
            "Epoch 185/300.. Learning rate: 0.0160.. Train loss: 0.3727.. Train acc: 0.8814.. Val loss: 1.6410.. Val acc: 0.6160\n",
            "Epoch 186/300.. Learning rate: 0.0158.. Train loss: 0.3704.. Train acc: 0.8838.. Val loss: 1.6206.. Val acc: 0.6149\n",
            "Epoch 187/300.. Learning rate: 0.0156.. Train loss: 0.3647.. Train acc: 0.8889.. Val loss: 1.6505.. Val acc: 0.6120\n",
            "Epoch 188/300.. Learning rate: 0.0153.. Train loss: 0.3618.. Train acc: 0.8874.. Val loss: 1.6266.. Val acc: 0.6177\n",
            "Epoch 189/300.. Learning rate: 0.0151.. Train loss: 0.3511.. Train acc: 0.8904.. Val loss: 1.6163.. Val acc: 0.6164\n",
            "Epoch 190/300.. Learning rate: 0.0148.. Train loss: 0.3507.. Train acc: 0.8907.. Val loss: 1.6493.. Val acc: 0.6137\n",
            "Epoch 191/300.. Learning rate: 0.0146.. Train loss: 0.3225.. Train acc: 0.8989.. Val loss: 1.6595.. Val acc: 0.6147\n",
            "Epoch 192/300.. Learning rate: 0.0144.. Train loss: 0.3161.. Train acc: 0.9012.. Val loss: 1.6332.. Val acc: 0.6232\n",
            "Epoch 193/300.. Learning rate: 0.0141.. Train loss: 0.3092.. Train acc: 0.9051.. Val loss: 1.6281.. Val acc: 0.6255\n",
            "Epoch 194/300.. Learning rate: 0.0139.. Train loss: 0.2928.. Train acc: 0.9081.. Val loss: 1.6223.. Val acc: 0.6227\n",
            "Epoch 195/300.. Learning rate: 0.0137.. Train loss: 0.2985.. Train acc: 0.9076.. Val loss: 1.6137.. Val acc: 0.6278\n",
            "Epoch 196/300.. Learning rate: 0.0134.. Train loss: 0.2884.. Train acc: 0.9118.. Val loss: 1.6255.. Val acc: 0.6225\n",
            "Epoch 197/300.. Learning rate: 0.0132.. Train loss: 0.2731.. Train acc: 0.9169.. Val loss: 1.6281.. Val acc: 0.6238\n",
            "Epoch 198/300.. Learning rate: 0.0130.. Train loss: 0.2734.. Train acc: 0.9168.. Val loss: 1.6373.. Val acc: 0.6213\n",
            "Epoch 199/300.. Learning rate: 0.0127.. Train loss: 0.2706.. Train acc: 0.9180.. Val loss: 1.6277.. Val acc: 0.6243\n",
            "Epoch 200/300.. Learning rate: 0.0125.. Train loss: 0.2729.. Train acc: 0.9145.. Val loss: 1.5736.. Val acc: 0.6343\n",
            "Epoch 201/300.. Learning rate: 0.0123.. Train loss: 0.2528.. Train acc: 0.9231.. Val loss: 1.6365.. Val acc: 0.6290\n",
            "Epoch 202/300.. Learning rate: 0.0121.. Train loss: 0.2433.. Train acc: 0.9268.. Val loss: 1.6187.. Val acc: 0.6273\n",
            "Epoch 203/300.. Learning rate: 0.0118.. Train loss: 0.2425.. Train acc: 0.9253.. Val loss: 1.6396.. Val acc: 0.6246\n",
            "Epoch 204/300.. Learning rate: 0.0116.. Train loss: 0.2243.. Train acc: 0.9344.. Val loss: 1.6628.. Val acc: 0.6203\n",
            "Epoch 205/300.. Learning rate: 0.0114.. Train loss: 0.2174.. Train acc: 0.9354.. Val loss: 1.5885.. Val acc: 0.6372\n",
            "Epoch 206/300.. Learning rate: 0.0112.. Train loss: 0.2063.. Train acc: 0.9378.. Val loss: 1.6011.. Val acc: 0.6344\n",
            "Epoch 207/300.. Learning rate: 0.0110.. Train loss: 0.2084.. Train acc: 0.9385.. Val loss: 1.6371.. Val acc: 0.6334\n",
            "Epoch 208/300.. Learning rate: 0.0107.. Train loss: 0.1960.. Train acc: 0.9432.. Val loss: 1.5832.. Val acc: 0.6396\n",
            "Epoch 209/300.. Learning rate: 0.0105.. Train loss: 0.1823.. Train acc: 0.9474.. Val loss: 1.5785.. Val acc: 0.6441\n",
            "Epoch 210/300.. Learning rate: 0.0103.. Train loss: 0.1844.. Train acc: 0.9486.. Val loss: 1.6190.. Val acc: 0.6349\n",
            "Epoch 211/300.. Learning rate: 0.0101.. Train loss: 0.1791.. Train acc: 0.9477.. Val loss: 1.6297.. Val acc: 0.6315\n",
            "Epoch 212/300.. Learning rate: 0.0099.. Train loss: 0.1694.. Train acc: 0.9517.. Val loss: 1.6511.. Val acc: 0.6337\n",
            "Epoch 213/300.. Learning rate: 0.0097.. Train loss: 0.1630.. Train acc: 0.9527.. Val loss: 1.5959.. Val acc: 0.6408\n",
            "Epoch 214/300.. Learning rate: 0.0095.. Train loss: 0.1560.. Train acc: 0.9556.. Val loss: 1.6036.. Val acc: 0.6384\n",
            "Epoch 215/300.. Learning rate: 0.0093.. Train loss: 0.1389.. Train acc: 0.9620.. Val loss: 1.6113.. Val acc: 0.6385\n",
            "Epoch 216/300.. Learning rate: 0.0091.. Train loss: 0.1342.. Train acc: 0.9634.. Val loss: 1.6012.. Val acc: 0.6428\n",
            "Epoch 217/300.. Learning rate: 0.0089.. Train loss: 0.1383.. Train acc: 0.9626.. Val loss: 1.6348.. Val acc: 0.6381\n",
            "Epoch 218/300.. Learning rate: 0.0087.. Train loss: 0.1245.. Train acc: 0.9671.. Val loss: 1.6249.. Val acc: 0.6428\n",
            "Epoch 219/300.. Learning rate: 0.0085.. Train loss: 0.1245.. Train acc: 0.9663.. Val loss: 1.5979.. Val acc: 0.6511\n",
            "Epoch 220/300.. Learning rate: 0.0083.. Train loss: 0.1133.. Train acc: 0.9696.. Val loss: 1.5957.. Val acc: 0.6455\n",
            "Epoch 221/300.. Learning rate: 0.0081.. Train loss: 0.1119.. Train acc: 0.9708.. Val loss: 1.5892.. Val acc: 0.6528\n",
            "Epoch 222/300.. Learning rate: 0.0079.. Train loss: 0.1039.. Train acc: 0.9730.. Val loss: 1.5927.. Val acc: 0.6483\n",
            "Epoch 223/300.. Learning rate: 0.0077.. Train loss: 0.1014.. Train acc: 0.9737.. Val loss: 1.5710.. Val acc: 0.6530\n",
            "Epoch 224/300.. Learning rate: 0.0075.. Train loss: 0.0901.. Train acc: 0.9776.. Val loss: 1.5762.. Val acc: 0.6537\n",
            "Epoch 225/300.. Learning rate: 0.0073.. Train loss: 0.0850.. Train acc: 0.9798.. Val loss: 1.5696.. Val acc: 0.6525\n",
            "Epoch 226/300.. Learning rate: 0.0071.. Train loss: 0.0795.. Train acc: 0.9809.. Val loss: 1.5737.. Val acc: 0.6538\n",
            "Epoch 227/300.. Learning rate: 0.0070.. Train loss: 0.0742.. Train acc: 0.9831.. Val loss: 1.5592.. Val acc: 0.6620\n",
            "Epoch 228/300.. Learning rate: 0.0068.. Train loss: 0.0662.. Train acc: 0.9848.. Val loss: 1.5560.. Val acc: 0.6565\n",
            "Epoch 229/300.. Learning rate: 0.0066.. Train loss: 0.0629.. Train acc: 0.9863.. Val loss: 1.5394.. Val acc: 0.6572\n",
            "Epoch 230/300.. Learning rate: 0.0064.. Train loss: 0.0590.. Train acc: 0.9879.. Val loss: 1.5416.. Val acc: 0.6603\n",
            "Epoch 231/300.. Learning rate: 0.0062.. Train loss: 0.0559.. Train acc: 0.9888.. Val loss: 1.5244.. Val acc: 0.6629\n",
            "Epoch 232/300.. Learning rate: 0.0061.. Train loss: 0.0569.. Train acc: 0.9882.. Val loss: 1.5298.. Val acc: 0.6618\n",
            "Epoch 233/300.. Learning rate: 0.0059.. Train loss: 0.0509.. Train acc: 0.9899.. Val loss: 1.5248.. Val acc: 0.6634\n",
            "Epoch 234/300.. Learning rate: 0.0057.. Train loss: 0.0463.. Train acc: 0.9914.. Val loss: 1.5187.. Val acc: 0.6621\n",
            "Epoch 235/300.. Learning rate: 0.0056.. Train loss: 0.0492.. Train acc: 0.9909.. Val loss: 1.5152.. Val acc: 0.6651\n",
            "Epoch 236/300.. Learning rate: 0.0054.. Train loss: 0.0456.. Train acc: 0.9919.. Val loss: 1.5089.. Val acc: 0.6668\n",
            "Epoch 237/300.. Learning rate: 0.0052.. Train loss: 0.0399.. Train acc: 0.9936.. Val loss: 1.4962.. Val acc: 0.6681\n",
            "Epoch 238/300.. Learning rate: 0.0051.. Train loss: 0.0363.. Train acc: 0.9942.. Val loss: 1.4907.. Val acc: 0.6681\n",
            "Epoch 239/300.. Learning rate: 0.0049.. Train loss: 0.0328.. Train acc: 0.9950.. Val loss: 1.5007.. Val acc: 0.6699\n",
            "Epoch 240/300.. Learning rate: 0.0048.. Train loss: 0.0324.. Train acc: 0.9950.. Val loss: 1.4805.. Val acc: 0.6705\n",
            "Epoch 241/300.. Learning rate: 0.0046.. Train loss: 0.0288.. Train acc: 0.9963.. Val loss: 1.4824.. Val acc: 0.6703\n",
            "Epoch 242/300.. Learning rate: 0.0045.. Train loss: 0.0281.. Train acc: 0.9968.. Val loss: 1.4752.. Val acc: 0.6741\n",
            "Epoch 243/300.. Learning rate: 0.0043.. Train loss: 0.0271.. Train acc: 0.9966.. Val loss: 1.4514.. Val acc: 0.6748\n",
            "Epoch 244/300.. Learning rate: 0.0042.. Train loss: 0.0237.. Train acc: 0.9973.. Val loss: 1.4539.. Val acc: 0.6763\n",
            "Epoch 245/300.. Learning rate: 0.0040.. Train loss: 0.0234.. Train acc: 0.9976.. Val loss: 1.4532.. Val acc: 0.6780\n",
            "Epoch 246/300.. Learning rate: 0.0039.. Train loss: 0.0237.. Train acc: 0.9970.. Val loss: 1.4581.. Val acc: 0.6747\n",
            "Epoch 247/300.. Learning rate: 0.0038.. Train loss: 0.0226.. Train acc: 0.9973.. Val loss: 1.4477.. Val acc: 0.6755\n",
            "Epoch 248/300.. Learning rate: 0.0036.. Train loss: 0.0211.. Train acc: 0.9980.. Val loss: 1.4400.. Val acc: 0.6811\n",
            "Epoch 249/300.. Learning rate: 0.0035.. Train loss: 0.0210.. Train acc: 0.9980.. Val loss: 1.4364.. Val acc: 0.6783\n",
            "Epoch 250/300.. Learning rate: 0.0034.. Train loss: 0.0203.. Train acc: 0.9980.. Val loss: 1.4433.. Val acc: 0.6796\n",
            "Epoch 251/300.. Learning rate: 0.0032.. Train loss: 0.0196.. Train acc: 0.9981.. Val loss: 1.4403.. Val acc: 0.6798\n",
            "Epoch 252/300.. Learning rate: 0.0031.. Train loss: 0.0191.. Train acc: 0.9981.. Val loss: 1.4287.. Val acc: 0.6785\n",
            "Epoch 253/300.. Learning rate: 0.0030.. Train loss: 0.0173.. Train acc: 0.9986.. Val loss: 1.4255.. Val acc: 0.6800\n",
            "Epoch 254/300.. Learning rate: 0.0028.. Train loss: 0.0182.. Train acc: 0.9984.. Val loss: 1.4240.. Val acc: 0.6817\n",
            "Epoch 255/300.. Learning rate: 0.0027.. Train loss: 0.0178.. Train acc: 0.9984.. Val loss: 1.4151.. Val acc: 0.6829\n",
            "Epoch 256/300.. Learning rate: 0.0026.. Train loss: 0.0174.. Train acc: 0.9985.. Val loss: 1.4165.. Val acc: 0.6803\n",
            "Epoch 257/300.. Learning rate: 0.0025.. Train loss: 0.0173.. Train acc: 0.9987.. Val loss: 1.4107.. Val acc: 0.6837\n",
            "Epoch 258/300.. Learning rate: 0.0024.. Train loss: 0.0165.. Train acc: 0.9987.. Val loss: 1.4073.. Val acc: 0.6816\n",
            "Epoch 259/300.. Learning rate: 0.0023.. Train loss: 0.0153.. Train acc: 0.9991.. Val loss: 1.4056.. Val acc: 0.6841\n",
            "Epoch 260/300.. Learning rate: 0.0022.. Train loss: 0.0159.. Train acc: 0.9989.. Val loss: 1.4037.. Val acc: 0.6842\n",
            "Epoch 261/300.. Learning rate: 0.0021.. Train loss: 0.0151.. Train acc: 0.9991.. Val loss: 1.4059.. Val acc: 0.6855\n",
            "Epoch 262/300.. Learning rate: 0.0020.. Train loss: 0.0151.. Train acc: 0.9992.. Val loss: 1.4013.. Val acc: 0.6855\n",
            "Epoch 263/300.. Learning rate: 0.0019.. Train loss: 0.0149.. Train acc: 0.9991.. Val loss: 1.4070.. Val acc: 0.6851\n",
            "Epoch 264/300.. Learning rate: 0.0018.. Train loss: 0.0145.. Train acc: 0.9992.. Val loss: 1.3992.. Val acc: 0.6859\n",
            "Epoch 265/300.. Learning rate: 0.0017.. Train loss: 0.0145.. Train acc: 0.9992.. Val loss: 1.4011.. Val acc: 0.6849\n",
            "Epoch 266/300.. Learning rate: 0.0016.. Train loss: 0.0141.. Train acc: 0.9992.. Val loss: 1.3931.. Val acc: 0.6857\n",
            "Epoch 267/300.. Learning rate: 0.0015.. Train loss: 0.0144.. Train acc: 0.9992.. Val loss: 1.3928.. Val acc: 0.6875\n",
            "Epoch 268/300.. Learning rate: 0.0014.. Train loss: 0.0141.. Train acc: 0.9993.. Val loss: 1.3938.. Val acc: 0.6859\n",
            "Epoch 269/300.. Learning rate: 0.0013.. Train loss: 0.0139.. Train acc: 0.9993.. Val loss: 1.3926.. Val acc: 0.6857\n",
            "Epoch 270/300.. Learning rate: 0.0012.. Train loss: 0.0135.. Train acc: 0.9994.. Val loss: 1.3931.. Val acc: 0.6877\n",
            "Epoch 271/300.. Learning rate: 0.0011.. Train loss: 0.0134.. Train acc: 0.9994.. Val loss: 1.3878.. Val acc: 0.6880\n",
            "Epoch 272/300.. Learning rate: 0.0011.. Train loss: 0.0135.. Train acc: 0.9992.. Val loss: 1.3871.. Val acc: 0.6842\n",
            "Epoch 273/300.. Learning rate: 0.0010.. Train loss: 0.0136.. Train acc: 0.9992.. Val loss: 1.3810.. Val acc: 0.6885\n",
            "Epoch 274/300.. Learning rate: 0.0009.. Train loss: 0.0136.. Train acc: 0.9991.. Val loss: 1.3838.. Val acc: 0.6870\n",
            "Epoch 275/300.. Learning rate: 0.0009.. Train loss: 0.0136.. Train acc: 0.9991.. Val loss: 1.3868.. Val acc: 0.6852\n",
            "Epoch 276/300.. Learning rate: 0.0008.. Train loss: 0.0127.. Train acc: 0.9993.. Val loss: 1.3825.. Val acc: 0.6878\n",
            "Epoch 277/300.. Learning rate: 0.0007.. Train loss: 0.0132.. Train acc: 0.9992.. Val loss: 1.3833.. Val acc: 0.6876\n",
            "Epoch 278/300.. Learning rate: 0.0007.. Train loss: 0.0130.. Train acc: 0.9992.. Val loss: 1.3829.. Val acc: 0.6882\n",
            "Epoch 279/300.. Learning rate: 0.0006.. Train loss: 0.0131.. Train acc: 0.9994.. Val loss: 1.3837.. Val acc: 0.6882\n",
            "Epoch 280/300.. Learning rate: 0.0005.. Train loss: 0.0132.. Train acc: 0.9994.. Val loss: 1.3791.. Val acc: 0.6889\n",
            "Epoch 281/300.. Learning rate: 0.0005.. Train loss: 0.0129.. Train acc: 0.9995.. Val loss: 1.3818.. Val acc: 0.6887\n",
            "Epoch 282/300.. Learning rate: 0.0004.. Train loss: 0.0130.. Train acc: 0.9994.. Val loss: 1.3827.. Val acc: 0.6873\n",
            "Epoch 283/300.. Learning rate: 0.0004.. Train loss: 0.0129.. Train acc: 0.9992.. Val loss: 1.3785.. Val acc: 0.6893\n",
            "Epoch 284/300.. Learning rate: 0.0004.. Train loss: 0.0125.. Train acc: 0.9993.. Val loss: 1.3765.. Val acc: 0.6891\n",
            "Epoch 285/300.. Learning rate: 0.0003.. Train loss: 0.0123.. Train acc: 0.9995.. Val loss: 1.3759.. Val acc: 0.6884\n",
            "Epoch 286/300.. Learning rate: 0.0003.. Train loss: 0.0127.. Train acc: 0.9993.. Val loss: 1.3844.. Val acc: 0.6869\n",
            "Epoch 287/300.. Learning rate: 0.0002.. Train loss: 0.0124.. Train acc: 0.9994.. Val loss: 1.3762.. Val acc: 0.6873\n",
            "Epoch 288/300.. Learning rate: 0.0002.. Train loss: 0.0122.. Train acc: 0.9994.. Val loss: 1.3810.. Val acc: 0.6890\n",
            "Epoch 289/300.. Learning rate: 0.0002.. Train loss: 0.0126.. Train acc: 0.9995.. Val loss: 1.3785.. Val acc: 0.6882\n",
            "Epoch 290/300.. Learning rate: 0.0001.. Train loss: 0.0125.. Train acc: 0.9995.. Val loss: 1.3825.. Val acc: 0.6891\n",
            "Epoch 291/300.. Learning rate: 0.0001.. Train loss: 0.0127.. Train acc: 0.9993.. Val loss: 1.3757.. Val acc: 0.6882\n",
            "Epoch 292/300.. Learning rate: 0.0001.. Train loss: 0.0125.. Train acc: 0.9994.. Val loss: 1.3775.. Val acc: 0.6886\n",
            "Epoch 293/300.. Learning rate: 0.0001.. Train loss: 0.0124.. Train acc: 0.9994.. Val loss: 1.3848.. Val acc: 0.6864\n",
            "Epoch 294/300.. Learning rate: 0.0001.. Train loss: 0.0121.. Train acc: 0.9995.. Val loss: 1.3807.. Val acc: 0.6883\n",
            "Epoch 295/300.. Learning rate: 0.0000.. Train loss: 0.0123.. Train acc: 0.9995.. Val loss: 1.3809.. Val acc: 0.6870\n",
            "Epoch 296/300.. Learning rate: 0.0000.. Train loss: 0.0126.. Train acc: 0.9995.. Val loss: 1.3768.. Val acc: 0.6877\n",
            "Epoch 297/300.. Learning rate: 0.0000.. Train loss: 0.0121.. Train acc: 0.9995.. Val loss: 1.3819.. Val acc: 0.6878\n",
            "Epoch 298/300.. Learning rate: 0.0000.. Train loss: 0.0126.. Train acc: 0.9994.. Val loss: 1.3799.. Val acc: 0.6904\n",
            "Epoch 299/300.. Learning rate: 0.0000.. Train loss: 0.0123.. Train acc: 0.9995.. Val loss: 1.3769.. Val acc: 0.6889\n",
            "Epoch 300/300.. Learning rate: 0.0000.. Train loss: 0.0125.. Train acc: 0.9993.. Val loss: 1.3788.. Val acc: 0.6884\n",
            "Image to be saved: lr=0.05_epochs=30088.png\n",
            "Test loss:  1.3526602706909179\n",
            "Test acc:  0.6865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wd with gamma=0.0001"
      ],
      "metadata": {
        "id": "06i4lLkQU-Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 300 \\\n",
        "--lr 0.05 \\\n",
        "--wd 0.0001 \\\n",
        "--lr_scheduler \\\n",
        "--seed 0 \\\n",
        "--fig_name lr=0.05_epochs=300.png_wd=0.0001 \\\n",
        "--test\n"
      ],
      "metadata": {
        "id": "SheIx_NAWv8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "tWaEZ8vWW7ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixup"
      ],
      "metadata": {
        "id": "XVhtIRo2XFu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Jose_dl/mainfile.py \\\n",
        "--dataset_dir ./mydata2/ \\\n",
        "--batch_size 128 \\\n",
        "--epochs 300 \\\n",
        "--lr 0.05 \\\n",
        "--wd 0.0005 \\\n",
        "--lr_scheduler \\\n",
        "--mixup \\\n",
        "--alpha 0.2 \\\n",
        "--seed 0 \\\n",
        "--fig_name final_image.png \\\n",
        "--test"
      ],
      "metadata": {
        "id": "B1Nq6Ai0bnEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}